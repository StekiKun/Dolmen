////
 This is the chapter on building lexical analyzers in the Dolmen documentation.
 Its master file is manual.adoc.
////

:jls8: https://docs.oracle.com/javase/specs/jls/se8
:jdk8: https://docs.oracle.com/javase/8/docs/api/java
:jdoc-base: {doc-base}/javadoc
:jdoc-dolmen: {jdoc-base}/org/stekikun/dolmen
:codegen: {jdoc-dolmen}/codegen
:lexbuf: {codegen}/LexBuffer.html
:debug: {jdoc-dolmen}/debug
:json-pdf: http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf
:ocaml: link:https://ocaml.org/index.fr.html[OCaml]
:antlr: link:https://www.antlr.org/[ANTLR]
:javacc: link:https://javacc.github.io/javacc/[JavaCC]
:jflex: link:https://jflex.de/[JFlex]

[#Lexers]
== Lexical Analyzers

////
 Plan:
   + Simple introductory example
     * Lexer structure
     * Main entry
     * Compiling and testing
     * Adding nested entries
     * Improving error reports
     * Debugging locations
   + Lexer entries
     * Regular expressions
     * Semantic actions
     * eof, _, orelse
     * options
     * CLI
   + Advanced concepts
     * nested rules
     * handling token locations
     * tail-recursion
   + Lexer description reference
////

In this chapter, we explain how to produce lexical analysers with
Dolmen by writing _Dolmen lexer descriptions_. We start by showing how
to write a simple lexer description for the JSON format before
presenting the various features of the lexer description format in
more detailed fashion. A complete reference for the lexer description
syntax can be found at <<Lexers_Syntax_Ref,the end of this chapter>>.

// The tutorial section
include::lexers-tutorial.adoc[]

<<<
=== Lexer Entries

The main building blocks of a Dolmen lexical analyzer are the lexer
_entries_. Dolmen will generate a single Java method per entry in the
resulting generated lexical analyzer. The syntax of a lexer entry is
as follows:

[listing, subs="{bnf-listing}"]
--
[[InJLEntry]]Entry :=
  (*public* | *private*)         // entry's visibility
  <<JLACTION,ACTION>>            // entry's return type
  *rule* <<JLIDENT,IDENT>>       // entry's name
  (<<JLACTION,ACTION>>)?         // entry's optional arguments
  *=* (*shortest*)?     // whether shortest or longest match rule is used
  <<JLClause,Clause>>+
--

An entry has a visibility (public or private), a return type, a name
and optional parameters: all of these will propagate directly to the
Java method generated from the entry. The remainder of the entry
is a sequence of _clauses_ which associate a regular expression
to some arbitrary Java _semantic action_:

[listing, subs="{bnf-listing}"]
--
[[InJLClause]]Clause :=
| *|* <<JLRegular,Regular>> <<JLACTION,ACTION>>
| *|* *orelse*  <<JLACTION,ACTION>>
--

TIP: Instead of a regular expression, a clause can be introduced
     with the special `orelse` keyword, whose meaning is explained
     in a <<Lexers_Wildcards,dedicated section below>>.

In essence, Dolmen lexers let you conveniently program methods which
consume an input sequence of characters, recognize some input patterns
described by regular expressions, and take on different actions
depending on what patterns have been matched. Whether you use these
actions to tokenize the input stream, to transform the input stream by
extracting, decoding, encoding or filtering parts of it, to count
occurrences of certain patterns or anything else, is completely
irrelevant to Dolmen. Dolmen's job is to take the high-level lexer
description and turn it into an efficient analyzer by taking care of
important details such as:

* managing input loading and buffering;

* compiling an entry's clauses into efficient code which matches
  input and select the adequate semantic action;

* keeping track of matched input positions, as well as those fragments
  corresponding to patterns captured via the `... as c` construct;

* assisting users by statically detecting and reporting common
  mistakes in lexer descriptions, such as useless clauses, etc.

When more than one clause in the entry matches the current input, the
default disambiguation rule is to pick the clause which yields the
_longest_ match. This behaviour can be changed for a given entry by
using the `*shortest*` keyword prior to the clauses: in this case,
priority is given to the clause yielding the shortest match. In any
case, when several clauses yield matches with the same length, the
clause which appears first in the lexer description will be selected.

The shortest-match rule is seldom used, at least for tokenizing
purposes. It can prove useful when writing text stream processors, in
particular when reading input from a network staream or from standard
input, as entries which use the shortest-match rule never need to
"look ahead" in the input stream and backtrack before selecting the
correct clause, unlike regular longest-match entries.

[#Lexers_Regular_Expressions]
==== Regular Expressions

In this section, we describe the Dolmen syntax for regular
expressions. Regular expressions appear as part of clauses in lexer
entries, but auxiliary regular expressions can also be defined
before the lexer entries:

[listing, subs="{bnf-listing}"]
--
[[InJLDefinition]]Definition :=
  <<JLIDENT,IDENT>> *=* <<JLRegular,Regular>> *;*
--

The identifier on the left-hand side in a regular expression
definition is the name which can be used to denote that regular
expression in subsequent definitions as well as in clauses. The
various regular expression constructs are summarized in
<<Lexers_Regexps_Table,this table>>, we detail each of them
in the following.

Literals::

The most basic form of regular expression are the _character_ and
_string_ literals. A character literal such as `'a'` or `'\040'` is a
regular expression which simply matches the given character.
Similarly, a string literal such as `"foo"` or `"\r\n"` is a regular
expression which matches the given string. The exact syntax for
character and string literals is detailed in the the
<<Lexers_Lexical_Conventions,lexical conventions>>.
+
[source,jl]
----
// Some literal regular expressions
space = ' ';
pub = "public";
----
+
Additionally, there are two "special" literals `+_+` and `eof`,
respectively called _wildcard_ and _end-of-input_. `eof` is
a meta-character which represents the fact that the end of
the input stream has been reached; in particular it does not
really match a given character, but simply succeeds when the
input stream has been exhausted. On the other hand, the
wildcard `+_+` will match any single character from the input,
and thus in particular is mutually exclusive with `eof`.
These special literals are discussed further
<<Lexers_Wildcards,below>>.

Character classes::
+
A _character class_ is a way of denoting a regular expression which
must only match a character in a subset of the whole character set. It
is introduced by square brackets `[...]` and can contain any non-empty
sequence of:
+
--
* single character literals, such as `'a'`, `'0'`;

* character ranges, such as `'a'-'z'`, `'0'-'9'`, denoting all the
  characters which belong to the specified range (inclusive on both
  ends);

* names of already defined regular expressions whose value corresponds
  to a character class (or can be reduced to a character class, such
  as a single character literal or the wildcard `+_+`).
--
+
The meaning of a character class can also be completely *inverted*
if it starts with a `^` character, in which case it matches any
character which does not belong to the specified set.
+
[source,jl]
----
// Some character classes
whitespace = ['\t' space '\f'];
alpha = ['a'-'z' 'A'-'Z'];
nonctrl = [^'\000'-'\037'];
----
The grammar for character classes is recalled in the
<<JLCharClass,syntax reference>>.

Character class difference::
The _difference operator_ `r # s` lets one define a regular
expression by taking the set of characters which match
the given class `r` but do not match the class `s`. The regular
expressions used as operands for `#` must reduce to character
classes or a lexical error will be reported by Dolmen.
+
[source,jl]
----
// Some character class differences
digit = ['0'-'9'];
nzdigit = digit # '0';
lowercase = alpha # ['a'-'z'];
ctrl = _ # nonctrl;
----

Repetitions::
+
Dolmen supports the following traditional postfix _repetition_
operators:
+
--
* `r?` matches zero or one occurrence of the regular expression `r`;
* `r+` matches one or more occurrences of the regular expression `r`;
* `r*` matches any number of occurrences of the regular expression
  `r`, including zero.
--
+
Besides these basic operators, it is also possible to specify any
finite number of repetitions using the following syntax where `n` and
`m` stand for literal decimal natural integers:
+
--
* `r<n>` matches exactly `n` occurrences of the regular expression `r`;
* `r<n,m>` matches anything between `n` and `m` (inclusive) occurrences
  of the regular expression `r`.
--
+
Note that `r<n>` is just syntactic shortcut for `r<n,n>`, and that
`r?` is completely equivalent to `r<0,1>`. There is no way to specify
an "infinite" upper bound, so `r+` and `r*` cannot be obtained with an
instance of `r<n,m>`. This is not a limitation in practice, since
something like any number of repetitions larger or equal to 3 can be
obtained by a combination such as `r<3>r*`.
+
[source,jl]
----
// Using repetition operators
decimal = digit+;
hexcode = [digit 'a'-'f' 'A'-'F']<4>;
word = ['_' alpha]+;
blanks = whitespace*;
----

Concatenation::
+
The concatenation of two regular expressions `r s` is a regular
expression which first matches `r` and then `s`. Concatenation is
_right-associative_, and has strictly lower precedence than every
operator we have seen so far. In particular, a string literal regular
expression such as `"foo"` is just syntactic sugar for the
concatenation `'f''o''o'` of its character literals. Similarly, a
repetition `r<4>` is just another, shorter, way of writing `r r r r`.
+
[source,jl]
----
// Using concatenation
float = decimal ('.' decimal)?;
string = '"' [^ ctrl '"']* '"';
r1 = 'a''b'+;  // ab......b
r2 = ('a''b')+;  // abab...ab
----
+
Note how parentheses can be used to disambiguate or regroup regular
expressions when natural precedence rules would not yield the intended
result.

Choice::
+
The _choice operator_ `r | s` matches anything that either regular
expression `r` or `s` matches. The choice operator is
_right-associative_, and has strictly lower precedence than every
operator we have seen so far, including concatenation. In particular,
the regular expression `r?` is equivalent to `r | ""`.
+
[source,jl]
----
// Using the choice operator
ident = alpha (alpha | digit | '_')*;
newline = '\r' | '\n' | "\r\n";
r3 = 'a'|'b''c';  // a or bc
r4 = ('a'|'b')'c';  // ac or bc
----

Capturing groups::
+
The _capture_ operator `r *as* c` matches exactly like the regular
expression `r` but when successful, it also saves the part of the
input stream which it matched. The matched contents are then made
available in the semantic action of the corresponding clause as a
local variable with name `c`. Captures are very convenient when
the semantic action would otherwise need to extract some interesting
part of the matched input, potentially at the cost of rescanning
some of it.
+
The type of a capture variable depends on the captured regular
expression, and on the overall clause's expression it appears in. It
can be either `String`, `char`, `Optional<String>` or
`Optional<Character>`. The `Optional` wrappers are introduced when the
overall regular expression can succeed without the captured regular
expression having matched at all, such as in `(r *as* c)?` or `(r1
*as* c) | r2`. Other than that, a capture variable is normally of type
`String`, unless it captures a regular expression which can only
statically match a _single_ character (e.g. a character literal, a
character class, a wildcard or an alternation of any of those), in
which case its type is that of a Java character.
+
====
The rationale behind the fact that the type of capture variables
adapts to the regular expression is twofold:

. Using a simpler type such as `char` in comparison to `String`
  enhances efficiency since no heap-allocation is required to capture
  a single character. Moreover, captured groups are always assigned
  before entering a clause's semantic action, even if it is never
  going to be used in the semantic action.

. Wrapping optional captured groups with `java.util.Optional` goes
  against raw efficiency instead, but for the very good reason of
  making sure that semantic actions always use captured variables in a
  way that is consistent with the associated clause. In particular,
  using `null` or default values instead would make it very easy
  for a developer to break a semantic action's intended behaviour
  by only slightly changing the associated regular expression.
  With our approach, such a breaking change will normally result
  in a type-checking error in the compiler.
====
+
Dolmen does not enforce that a capture variable appears only
once in a clause's regular expression. This allows for instance
patterns such as `(r1 *as* c) | (r2 *as* c)` where `c` can
then be handled in a uniform fashion in the semantic action.
The type of a capture variable which appears more than once
in a regular expression is simply the "simplest" type which
can accomodate all occurrences of the variable. For instance,
the capture variable can only have type `char` if all its
occurrences can have type `char`, and so on.
+
[source,jl]
.Examples of captures in action
----
// North-American style phone number, capturing groups
phone = (digit<3> as area) '-' (digit<3> as office) '-' (digit<4> as num);
// Markdown-style header, capturing the title
header = '#'+ blanks ([^ '\r' '\n']+ as title) newline;
// GCC #line directive, capturing filename and line number
linedir = "#line" blanks (digit+ as line) blanks (string as filename);
// Java Unicode escape sequence, capturing the code unit value
unicode = '\\' 'u'+ (hexcode as code);
----
+
Even when a capture variable is bound only once in a regular
expression, it is possible that it is part of a repeated expression,
for instance `(r *as* c)<2>`, and can therefore be matched several
times during a single overall match. In such a case, the variable will
be bound to the contents that correspond to the last match. Similarly,
if the same capture variable appears nested in a captured group, such
as in `(r0 (r1 *as* c) r2) *as* c`, only the outermost occurrence can
ever find its way into the semantic action. The innermost captures are
actually optimized away very early by Dolmen, and do not even
contribute to the choice of the type associated to the variable `c`.
+
Finally, there may be cases where there is not a unique way of
matching the input stream with some regular expression, and that the
different ways would yield different captured contents. For instance,
consider matching the string `aaaaa` with the regular expression
`('a'<2,3> *as* i) ('a'<2,3> *as* j)`: there are two correct ways in
which `i` and `j` could be assigned. In such a case, which one is
produced by the Dolmen-generated lexical analyzer is left unspecified.


The following table summarizes the different regular expression
constructs in order of *decreasing* precedence, i.e. operators
appearing first bind tighter than those appearing later down
the table.

[[Lexers_Regexps_Table]]
.Regular expression operators
[cols=4*,options="header"]
|===
| Name
| Syntax
| Meaning
| Associative

| Difference
| `r # s`
| Matches the characters in `r` which are not in `s`.
  Both `r` and `s` must reduce to character classes.
| No

| Option
| `r?`
| Matches like `r`, or the empty string.
| No

| Kleene star
| `r*`
| Matches any number of repetitions of `r`.
| No

| Kleene plus
| `r+`
| Matches one or more number of repetitions of `r`.
| No

| Repetition
| `r<n>`
| Matches exactly `n` repetitions of `r`.
| No

| Bounded repetition
| `r<n, m>`
| Matches between `n` and `m` (inclusive) repetitions of `r`.
| No

| Concatenation
| `r s`
| Matches `r` first and then `s`.
| Right-associative

| Choice
| `r \| s`
| Matches either `r` or `s`.
| Right-associative

| Capture
| `r as c`
| Matches like `r` but associates the matched part
  to a local variable named `c`.
| Left-associative

|===

[#Lexers_Semantic_Actions_Ref]
==== Semantic Actions

When a matching clause has been chosen by the lexing engine and the
corresponding prefix of the input consumed, the associated semantic
action is executed. The semantic action can be almost arbitrary Java
code, but must respect a few principles.

. The semantic action must be such that all paths are guaranteed
  to adequately exit the control-flow, which means that every
  execution path must end with either of the following:

  ** a `return` statement, compatible with the declared
    return type of the enclosing entry;
  
  ** the throwing of an uncaught exception (preferably a
     link:{codegen}/LexBuffer.LexicalError.html[`LexicalError`]
     exception);

  ** a Java `continue lbl` statement to restart the current lexer
     entry `lbl` without actually calling the Java method implementing
     the entry (cf. <<Lexers_Tail_Recursion_Ref,tail-recursion>>);
     this works because Dolmen always generates the code of a lexer
     entry in a loop with a
     {jls8}/html/jls-14.html#jls-LabeledStatement[labeled statement]
     whose label is exactly the name of the entry.

+
When this principle is not respected, the generated code may
contain errors or warnings, and the behaviour of the generated
lexical analyzer is not predictable.

. The generated lexer is a subclass of the
link:{codegen}/LexBuffer.html[`LexBuffer`] class from the Dolmen
runtime, and as such has access to a number of fields and methods
inherited from `LexBuffer`. Some of these fields and methods are
provided primarily for use by user code in semantic actions, whereas
others deal with the internal functionality of the lexing engine and
should *not* be called or tampered with in semantic actions. The
reader may wonder why these fields and methods which are reserved for
internal use are declared as `protected`; they cannot be made
`private` in `LexBuffer` because they must be accessed from the
code of the subclass generated by Dolmen itself.
+

In order to document which protected members are intended for user
code and which are off limits, the
link:{codegen}/DolmenInternal.html[`DolmenInternal`] annotation has
been introduced and is used to annotate every member which is declared
as `protected` but should not be accessed from user code (i.e. either
from semantic actions or from the prelude and postlude).  As a
refinement, a field can be annotated with `@DolmenInternal(read = true)`
in which case access to the field from user code is tolerated as long
as the value is not modified. In this context, "modifying" a field
encompasses both modifying the field itself, and in the case the field
is a reference object, modifying the actual value of the referred object
without changing the reference itself. For instance, the following
declaration is possible:
+
[source,java]
----
@DolmenInternal(read = true)
protected final Set<Integer> numbers;
----
+
It signifies that `numbers` can be read from user code--the field
itself cannot be modified as it is final anyway--but only methods
which do not change the contents of `numbers` should be allowed.  As
the Java language lacks a proper notion of `const`-ness, it cannot be
expressed in method signatures whether they observationally change
their receiver's state or not so one must rely on documentation and
common sense in practice.
+
When internal members are used in semantic actions, the lexical
analyzer may behave unexpectedly as this can break some of the
internal invariants it relies on. Even if things seem to work fine,
such code can be broken by a future Dolmen update.

We conclude this section on semantic actions with an exhaustive
presentation of the fields and methods from
link:{codegen}/LexBuffer.html[`LexBuffer`] which are available for use
in semantic actions. One can always refer to the Javadoc associated
to these members, either via the link:{codegen}/LexBuffer.html[HTML pages]
or using your IDE when accessing the Dolmen runtime.

:field-pro: image:field_protected_obj.png[title="protected field"]
:meth-pro: image:methpro_obj.png[title="protected method"]
:meth-pub: image:methpub_obj.png[title="public method"]

Input position::
  The following members available in semantic actions relate
  to the management of the input position.      
+
--
{field-pro} `String filename`::
            This field contains the _name_ of the current input
            being analyzed by the lexing engine. Despite the field's name,
            this name need not be an actual filename, it is only used in
            positions and error reporting. +
            It is not final as it may be
            changed for instance when handling a GCC-like `#line`
            directive.
{field-pro} `Position startLoc`::
            This field contains the starting position of the current lexeme. +
            It can be modified before returning from a semantic action, e.g.
            for adjusting <<Lexers_Token_Locations,token locations>> when
            the lexical analyzer is used by a parser.
{field-pro} `Position curLoc`::
            This field contains the ending position of the current lexeme,
            i.e. the position of the first character that _follows_ the
            lexeme. +
            It can be modified before returning from a semantic action, e.g.
            for adjusting <<Lexers_Token_Locations,token locations>> when
            the lexical analyzer is used by a parser.
{meth-pro} `void newline()`::
           Modifies the internal line and column counter to account
           for the consumption of a line terminator. This needs to be
           called in semantic actions accordingly if one wants to keep
           correct track of lines and columns.                 
{meth-pro} `<T> T saveStart(Supplier<T> s)`::
           This convenience method calls the given supplier `s`, returning
           the same value as `s`, but saving the current value of `startLoc`
           before the call and restoring it afterwards. This is used when
           <<Lexers_Nested_Rules,nested rules>> are used to parse a token
           and the token location needs to be adjusted.
{meth-pro} `<T> T savePosition(Supplier<T> s, Position start)`::
           This convenience method calls the given supplier `s`, returning
           the same value as `s`, but setting the value of `startLoc` to
           `start` afterwards. This is used when
           <<Lexers_Nested_Rules,nested rules>> are used to parse a token
           and the token location needs to be adjusted.
{meth-pro} `void saveStart(Runnable r)`::
           This convenience method calls the given runnable `r`, saving
           the current value of `startLoc` before the call and restoring
           it afterwards. This is used when
           <<Lexers_Nested_Rules,nested rules>> are used to parse a token
           and the token location needs to be adjusted.
{meth-pro} `void savePosition(Runnable r, Position start)`::
           This convenience method calls the given runnable `r` and sets
           the value of `startLoc` to `start` afterwards. This is used when
           <<Lexers_Nested_Rules,nested rules>> are used to parse a token
           and the token location needs to be adjusted.
--

Lexeme::
  The following members available in semantic actions relate
  to the retrieval of the last matched lexeme. They all refer
  to the part of the input which was last matched by a lexer
  clause; in the case where there have been calls to a 
  <<Lexers_Nested_Rules,nested rule>>, these methods will return
  information relevant to the last matched clause, and not to
  the lexeme which was matched by the clause of the _current_
  semantic action.

+
--
{meth-pro} `String getLexeme()`::
           Returns the last matched lexeme.
{meth-pub} `Position getLexemeStart()`::
           Returns the starting position of the last matched lexeme.
{meth-pub} `Position getLexemeEnd()`::
           Returns the end position of the last matched lexeme. By convention,
           this is the position of the first character that _follows_ the
           lexeme.
{meth-pub} `int getLexemeLength()`::
           Returns the length of the last matched lexeme.
{meth-pro} `char getLexemeChar(int idx)`::
           Returns the character at index `idx` in the last matched lexeme.
           The index is 0-based, and must be between 0 (inclusive) and
           `getLexemeLength()` (exclusive).
{meth-pro} `CharSequence getLexemeChars()`::
           Returns the contents of the last matched lexeme as a character
           sequence. The character sequence refers to the current internal
           state of the lexing engine and becomes unspecified after 
           <<Lexers_Nested_Rules,nested rule calls>>, even if it was retrieved
           before any nested call.
{meth-pro} `void appendLexeme(StringBuilder buf)`::
           Appends the contents of the last matched lexeme to the
           given buffer `buf`.
--

[#Lexers_Input_Management]
Input management::
  The following members available in semantic actions
  relate to the management of the input stream during a lexical
  analysis. The lexing engine supports switching input streams
  in the middle of an analysis, and can manage an _input stack_
  to let users implement inclusion directives like GCC's `#include`
  or TeX's `\input`.
+
--
{meth-pro} `void changeInput(String fn, Reader reader)`::
           This closes the current input stream and switches to the new
           input specified by `reader` and whose display name will be `fn`.
{meth-pro} `void pushInput(String fn, Reader reader)`::
           This pushes the current input stream on the input stack and
           start consuming the given `reader` instead. The stacked
           input stream can be resumed with `popInput()`.
{meth-pro} `boolean hasMoreInput()`::
           This returns whether the current stream is the last one
           remaining in the input stack. In other words, only when
           this method returns `true` can `popInput()` be used.
{meth-pro} `void popInput()`::
           This closes the current stream and resumes the last one
           which was pushed to the stack. Whether the stack is
           empty or not can be checked beforehand with `hasMoreInput`.
--

Error management::
  The following members available in semantic actions
  relate to error management during a lexical analysis.
+
--
{meth-pro} `LexicalError error(String msg)`::
           Returns a link:{codegen}/LexBuffer.LexicalError.html[`LexicalError`] exception
           with the given message string and reported at the current lexer position.
           This is the preferred way of reporting lexical errors in semantic actions.
           Note that unlike the other methods here, `error` is not final: it can be
           overriden so that one can for instance customize the way the error message
           is built and presented. In particular it could be internationalized if need be.
{meth-pro} `char peekNextChar()`::
           Returns the character which directly follows the last matched lexeme in
           the input stream, or 0xFFFF if end-of-input has been reached. This can be
           used to produce better error messages, or in some cases where a semantic
           action needs to look ahead at the stream without consuming it to decide
           what should be done.
{meth-pro} `int peekNextChars(char[] buf)`::
           Copies the characters which directly follow the last matched lexeme in
           the input stream to `buf`, and returns the number of characters actually
           copied. This can be used to produce better error messages, or in some cases
           where a semantic action needs to look ahead at the stream without consuming
           it to decide what should be done.
--

[#Lexers_Wildcards]
==== Understanding `eof`, `_` and `orelse`

In this section, we discuss the meaning of the special regular
expressions `eof` and `_`, which can sometimes be confusing, as well
as the special `orelse` keyword which can be used to introduce a
"catch-everything-else" clause in a lexer entry.

End-of-input::

As explained <<Lexers_Regular_Expressions,above>>, `eof` is a special
regular expression literal which matches when the input stream has
been exhausted. It actually represents the single meta-character
`\uFFFF`, which is used internally to represent the end-of-input and
is not a valid escaping sequence in character or string literals.
Even if `eof` is internally encoded as a character, it does not behave
as a character:

* matching `eof` does not _consume_ any character from the input
  stream, it simply checks that the end of the input stream has
  indeed been reached; therefore it always matches an empty
  input string, whereas regular characters always match an input
  of size 1;
* because it does not consume anything, `eof` has some rather
  exotic properties when it comes to regular expressions;
  for instance, `eof` and `eof+` are completely equivalent;
* `eof` is not considered as a regular expression which reduces to a
  character class; hence it is not possible to use `eof` in a
  character set difference operation `.. # ..`, or inside a character
  class `[.. eof ..]`;
* finally, even though `eof` stands for the end of the input stream,
  any regular expression which matches the empty string can still
  match after `eof`, e.g. `eof ""`, `eof ("" as c)` or even `eof
  ("foo"?)` will all successfully match the empty string `""`.

+
All in all, writing complex regular expressions using `eof` should be
really uncommon. As a rule of thumb, `eof` is usually best used all by
itself in a clause, as the end-of-input normally calls for some ad hoc
action, in particular in terms of <<Lexers_Error_Reports,error
reporting>>.

Wildcard::

The wildcard `+_+` is a regular expression which can match any valid
character. It is actually equivalent to the character class
`['\u0000' - '\uFFFE']`, and in particular *excludes* the end-of-input
meta-character. Therefore, `_` will behave in an expected way,
consuming exactly one character of the input stream, much like `.` in
many common regular expression engines.
+
What sometimes causes confusion is that `+_+` is not sufficient when
trying to write a default clause to catch any unexpected character for
instance:
+
[source,jl]
----
| _ as c { throw error("Unexpected character: " + c); }
----
+
This will catch any actual unexpected character but will still leave
`eof` unhandled--and if so, this will be reported by Dolmen as a
warning for a potentially incomplete lexer entry. It is possible to
match `(_ | eof) as c` or `(_ as c) | eof` of course, but whereas `c`
in the clause above had type `char`, `c` in the latter examples would
be an `Optional<Character>` which means the semantic action will not
handle both cases in a uniform manner anyway. It is usually simpler to
just have another clause dedicated to `eof`.
+
Note that for the same reason that `+_+` only matches actual non
end-of-input characters, the character class complement operation
`[^...]` only contains characters in the 0x0000-0xFFFE range. In
other words, the complement `[^ cset ]` of some character set
is actually equivalent to `_ # [cset]`.

Clause `orelse`::

The keyword `orelse` can be used in place of a regular expression to
introduce a special form of clause:
+
[source, jl]
----
| orelse  { ... }
----
+
The meaning of the `orelse` pattern depends on the context in which it
is used, namely of the other clauses declared before in the same lexer
entry. Its purpose is to match the longest possible prefix of input
which could not possibly match any of the other clauses above.
+
More precisely, suppose `orelse` is preceded by _n_ clauses with
respective regular expressions `r1`, ... `rn`; the regular expression
that `orelse` will denote is computed as follows:
+
--
. For each `ri`, compute the character set `Ci` of the characters
  which can start a match of `ri`. This character set can easily
  be computed by induction on the regular expression; for instance,
  it is the first character of the string for a string literal,
  the union of the first characters of `s` and `t` for a choice
  expression `s | t`, and so on.

. Compute the union `C` of all the character sets `Ci`, i.e. the set
  of all the characters which can start a match of any of the `ri`.

. Then, `orelse` will represent the regular expression `[^C]+`,
  i.e. it will match any non-empty number of characters which
  cannot start any of the `ri`.
--
+
An `orelse` clause is useful when one is only interested in
recognizing certain patterns, but can pass over the rest of the
input without having to interpret it. Consider for instance
a lexer entry `mlcomment` which is entered when starting
a (possibly multi-line) comment `/* ... */` and which must
match until the end of the comment:
+
[source,jl]
.An entry for multi-line comments
----
private { void } rule mlcomment =
| "*/"    { return; }
| newline { newline(); continue mlcomment; }
| eof     { throw error("Unterminated comment"); }
| ...
----
+
where `newline` is defined as `('\r' | '\n' | "\r\n")`.  As it is, the
entry recognizes the closing comment delimiter and also handles line
breaks and end-of-input gracefully. It still needs to be able to pass
everything else that the comment can contain. One correct way to do
that is to use a wildcard to catch everything else:
+
[source,jl]
----
...
| _     { continue mlcomment; }
----
+
In particular this works because by the longest-match rule, it will not
match a `*` character if it happens to be followed by a `/`. Unfortunately,
reentering the rule at every character is somewhat inefficient in comparison
to matching largest parts of the comment at once; indeed, every time a
rule is entered or reentered, the generated lexical analyzer performs some
minor bookkeeping regarding the current lexeme's positions. To pass as much
of the comment's contents as we possibly can in one go, we can use an ad-hoc
regular expression like the following:
+
[source,jl]
----
...
| [^'*' '\r' '\n']+     { continue mlcomment; }
----
+
The issue with this is twofold:
+
--
. On one hand, it is fragile as it relies on correctly listing
  all characters which are of interest to the other clauses in
  the rule. Should we forget one of them, the Kleene `+` operator
  and the longest-match rule will conspire to make sure our entry
  does not work as intended. Also, this must be maintained when
  adding new patterns to the rule.
. On the other hand, it still does not cover the case of `*`
  characters which are not followed by a `/`. One could of course
  fix the regular expression like this:
+
[source,jl]
----
...
| ([^'*' '\r' '\n']+ | '*'+[^'/' '*'])+  { continue mlcomment; }
----
+
but this would definitely make the first issue even more pregnant
(Hint: incidentally, this regular expression is still not good
enough!).
--
+
Reliability and reviewability are design principles in Dolmen, and the
`orelse` clause was brought up precisely to bring an elegant solution
for such a case. Using `orelse` here will be equivalent to `[^'*' '\r'
'\n']+` but in a much less error-prone way:
+
[source,jl]
----
private { void } rule mlcomment =
| "*/"    { return; }
| newline { newline(); continue mlcomment; }
| eof     { throw error("Unterminated comment"); }
| '*'     { continue mlcomment; }
| orelse  { continue mlcomment; }
----
+
Of course, lonely `*` characters still need to be singled out but this
is reasonably simple. One can even switch the last two clauses around
and use a wildcard:
+
[source,jl]
----
private { void } rule mlcomment =
...
| orelse  { continue mlcomment; }
| _       { continue mlcomment; }
----
+
Writing the entry this way will make maintenance really
straightforward. For instance, suppose we want to start
allowing nested `/* ... */` comments, this can be achieved
by keeping track of the current comment "depth" in a local
field and add a clause to handle opening delimiters:
+
[source,jl]
----
private { void } rule mlcomment =
| "*/"    { if (--depth == 0) return; continue mlcomment; }
| "/*"    { ++depth; continue mlcomment; }
| newline { newline(); continue mlcomment; }
| eof     { throw error("Unterminated comment"); }
| orelse  { continue mlcomment; }
| _       { continue mlcomment; }
----
+
The benefit of using `orelse` and a wildcard instead of complex
hand-crafted expressions is immediately apparent, as this is all that
we have to do to handle nested comments in our entry. `orelse` will
now take care of not consuming `/` characters, whereas the wildcard
will deal with `/` characters that are not followed by a `*`.
+
Note that as in the example above, the `orelse` clause need not
be the last one in the entry, it will simply depend on the clauses
defined above. It is even syntactically possible to have more than
one `orelse` clause in an entry, although all such extra clauses
will be shadowed by the first one by construction.

[#Lexers_Options]
==== Lexer Options

The lexical analyzer generator accepts some options which can
be specified right at the top of a lexer description, with
the following syntax:

[source,jl]
.Syntax for configuration options
--
[key1 = "value1"]
[key2 = "value2"]
...
--

A configuration option given as a key-value pair where the
key identifies the option being set, and the value is a
string literal. Unlike string literals in regular expressions,
option values can be multi-line string literals.

In the following, we list the different configuration options
available in lexer descriptions and document their effect.

class_annnotations::

The `class_annotations` option lets one specify arbitrary annotations
that will be added on the class generated by the lexical
analyzer. Indeed, a lexer description will result in the generation of
a single public class extending the link:{codegen}/LexBuffer.html[base
lexing buffer class] from the Dolmen runtime.
+
This can be useful in particular to add particular instances of
`@SuppressWarnings(...)` annotations. Dolmen tries very hard to
generate code without warnings--not counting warnings from the
semantic actions of course, at least for a reasonable default
configuration of the Java compiler. When working in a project with
stronger requirements, generated classes may have warnings and the
`class_annotations` option can be used to explicitly ignore these
warnings. For instance, to suppress warnings on missing Java
documentation in the generated lexer, one can use:
+
[source,jl]
----
[class_annotations = "@SuppressWarnings(\"javadoc\")"]
----
+
When referencing an annotation from another package, make sure it is
fully qualified or add the corresponding import to the imports section
of the lexer description, so that the annotation can be resolved in
the generated Java compilation unit. For instance, when working in a
project which uses the
link:https://help.eclipse.org/2019-12/index.jsp?topic=%2Forg.eclipse.jdt.doc.user%2Ftasks%2Ftask-using_null_annotations.htm[Eclipse
JDT null analysis], one may need something like this:
+
[source,jl]
----
[class_annotations = "@org.eclipse.jdt.annotation.NonNullByDefault"]
----
+
as Dolmen is implemented with the full null analysis enabled and uses
a non-null type default policy.


[#Lexers_CLI]
==== Command Line Interface

The Dolmen runtime is an executable Java archive which can be used as
a command-line tool to generate a lexical analyzer from a lexer
description. Another way of using Dolmen is via the companion
https://dolmenplugin.stekikun.org[Eclipse plug-in]; nonetheless, the
command line is useful in a variety of scenarios like building
scripts, Makefile, continuous integration, etc.

The command line interface can be invoked as follows:

[source,console]
----
$ java -jar Dolmen_1.0.0.jar -l ..options.. source
----

where `source` is the name of the source lexer description. The `-l`
option instructs Dolmen that the source is a lexer description and that
a lexical analyzer should be generated. The `-l` flag can be omitted
if the source file has the `.jl` extension. The other available
command line options are detailed in the following table.

.Command line options
[cols=4*,options="header"]
|===
| Name
| Type
| Default
| Description

| `-h/--help`
| Flag
| No
| Displays help about command line usage.

| `-q/--quiet`
| Flag
| No
| Quiet mode: suppresses all output except for error report.

| `-l/--lexer`
| Flag
| No
| Generates a lexical analyzer from the source file.

| `-o/--output`
| String
| Working directory
| Output directory for the generated class.

| `-c/--class`
| String
| Name of the source file
| Name of the generated class.

| `-p/--package`
| String
| *Required*
| Name of the package for the generated class.

| `-r/--reports`
| String
| Name of the source file + `.reports`
| No
| File where potential problems should be reported.

|===

String options must follow the option name directly, e.g.  `-o mydir
--class Foo` instructs Dolmen to generate the class `Foo.java` in the
`mydir` directory.  Flag values in their short form can be grouped
together. For instance, `-lq` instructs Dolmen to generate a lexical
analyzer in quiet mode. **Note that the `--package/-p` option is
required:** the generated class will be declared in the given package,
and Dolmen does not support generating classes in the _default_
package.

.Example 1
====
The following command will generate a lexical analyzer in
`src/foo/bar/MyLexer.java`, in the package `foo.bar`.
Only errors will be reported on the standard output, other
reports will be in `MyLexer.jl.reports`.

[source,console]
----
$ java -jar Dolmen_1.0.0.jar -q -p foo.bar -o src/foo/bar MyLexer.jl
----
====

.Example 2
====
The following command will generate a lexical analyzer in
`src/foo/bar/MyLexer.java`, in the package `foo.bar`.
The `-l` flag is required as Dolmen cannot recognize the
extension of the source description. +
Dolmen will output information on the different phases
of the lexical analyzer generation, including any errors.
Other problem reports will be written to `lexer.reports`.

[source,console]
----
$ java -jar Dolmen_1.0.0.jar -l -p foo.bar --reports lexer.reports src/foo/bar/MyLexer.lexer
----
====

=== Advanced Concepts

In the remaining sections, we take a quick tour of some advanced principles
lexical analysis with Dolmen. These concepts have all already been introduced,
in more or less depth, in the <<Lexers_Tutorial,tutorial>>.

[#Lexers_Nested_Rules]
==== Nested Rules

When trying to tokenize an input stream containing some sentences in a
programming language or some data in an non-binary exchange format, it
it usually apparent that the same characters may take on very
different meanings depending on the _context_ in which they
appear.

For instance, special symbols like `+` or `;` which represent
operators or separators in one part of the input, need not be
interpreted in any special manner when they appear inside a string
literal, or inside some comment. Similarly, a keyword loses its
special status should it belong to some comment or literal. Many
languages even embed some other language in their syntax, in one form
or another; consider for instance GCC's preprocessor directives which
are added on top of the C language, or more evidently the presence of
Java actions inside Dolmen lexer descriptions. Even simple traits such
as literals supporting a variety of escape sequences, or interpreted
documentation comments such as Javadoc or Doxygen, can be thought of
as "mini"-languages inside the main programming language.

The GCC preprocessor does not need to _parse_ the C declarations
between the preprocessor directives, no more than Dolmen actually
needs to _parse_ the semantic actions in Java, but both need to be
able to care for the lexical conventions of the embedded language.
Trying to write a lexical analyzer which can handle preprocessing
directives, complex literals, structured comments and embedded
snippets in some external language, without being able to effectively
separate the different concerns raised by these different contexts,
amounts to writing a single very complex entry rule whose clauses are
able to recognize everything. Imagine clauses like these:

[source,jl]
----
public { Token } rule main =
...
| '"' (string_literal as s) '"'   { return Token.STRING(decode(s)); }
| '{' (java_action as a) '}'      { return Token.ACTION(a); }
| '#' (pp_directive as d) '\n'    { handle(d); continue main; }
| "/*" ml_comments "*/"           { continue main; }
...
----

Let us first ignore how complex these regular expressions ought to be,
and just discuss each clause's purpose:

. The `string_literal` clause recognizes a literal string's contents,
  with possible escape sequences for control characters or Unicode
  code points. The escape sequences are _unescaped_ by the `decode`
  function in the semantic action before returning the token. One
  could store the literal without decoding the escape sequences, but
  this will need to be done at some point in time when the program
  needs to interpret or expose the literal's value.

. The `java_action` clause recognizes a Dolmen-style Java action
  delimited by curly braces. It turns out it is *impossible* to write
  such a regular expression because `{` and `}` can appear nested
  inside the Java actions and thus the language to recognize is not
  https://en.wikipedia.org/wiki/Regular_language[regular], but we
  could make do by picking more discriminating delimiters such as `{%`
  and `%}`, the way https://jflex.de/[JFlex] does. The `java_action`
  still needs to correctly analyze the Java lexicon, as it needs to be
  able to ignore occurrences of `%}` inside Java string literals or
  Java comments.

. The `pp_directive` clause recognizes some sort of preprocessing
  directive. As is often the case with preprocessing, it may not be
  part of a separate first phase, but may be applied on-the-fly during
  the lexical analysis. Here, we suppose the directive is directly
  handled by some method `handle` in the semantic action, before
  handing control back to the lexer. This method may register a new
  macro, push some new file on the <<Lexers_Input_Management,input
  stack>>, etc. It must decode and validate the raw directive which
  has been recognized.

. The `ml_comments` clause recognizes C-style multi-line comments.  It
  may contain structured comments aimed at tools like Javadoc or
  Doxygen, and in this case the clause simply skips the comment.  Not
  trying to interpret the contents of the comment, we can surely leave
  this task to external tools which will be applied independently on
  the source code. But what if we are precisely writing a
  documentation tool like these? In that case, not only do we need to
  handle and decode the contents of the structured comments, but we
  also need to recognize the remainder of the language with enough
  precision, so that it is possible for instance to ignore occurrences
  of `+/*+` appearing in a string literal. Even if we can avoid
  interpreting the comments, lexing a multi-line comments in one go
  means that line and column numbers cannot be properly updated
  (cf. link:{lexbuf}#newline--[newline()]), except by scanning for
  line terminators in the semantic action. Finally, if we ever wanted
  to support _nested comments_, such as `(* .. *)` comments in
  {ocaml}, then sadly it would not be possible to write a suitable
  `ml_comments` regular expression as nested comments are not a
  regular language.

These examples basically sum up the main reasons why we want some form
of contextual rules in lexer generation:

* writing and maintaining correct complex regular expressions
  like `java_action` or `string_literal` is precisely what lexer
  generators are supposed to help one avoid;
* even when recognized successfully, the internal structure of
  these lexemes must still be decoded eventually, and that task
  will somewhat duplicate the work that the generated lexer did
  when recognizing the expression; not only is it inefficient,
  it also duplicates the actual _logic_ of the internal structure;
* when the contents of a string literal or a Java action happen to be
  lexically invalid (e.g. because of an invalid escape sequence), the
  lexical error will be returned at the very beginning of the clause,
  which is not very helpful; the only way to provide a better error
  message is to write other clauses matching _invalid_ cases and
  throw custom error messages.  

Lexer generators like {antlr}, {javacc} or {jflex} allow
contextualization by introducing a notion of _state_ and letting
users associate different lexing rules to different states, as well
as switching states in semantic actions.

NOTE: Unlike {javacc} and {jflex} which call them "lexical states",
        {antlr} actually calls these states "modes", which is arguably
        a better way of characterizing them, as the lexical analyzer's
        _state_ is not reduced to the current mode. Instead the
        analyzer's state comprises the whole stack of modes which have
        been entered and are yet to be returned from.

Dolmen does not follow a state-based approach but a function-based
one, inspired by OCaml's lexer generator
link:https://caml.inria.fr/pub/docs/manual-ocaml/lexyacc.html[ocamllex]. In
this approach, lexing rules are grouped in different lexer _entries_
which each correspond to a different function in the generated
lexer. Each entry can correspond to a different lexing context, and
switching from one context to another is done by calling the adequate
entry. A lexer description could contain different public entries
which are designed to be called independently from an external class;
it can also feature a main entry and other private entries which are
only designed to be called from other entries' semantic actions. We
call such rules which are accessed via calls nested in other rules'
semantic actions _nested rules_.

We now demonstrate the use of nested rules on the examples of string
literals and nested comments.

.Example 1 (String literals)
====
Let us use nested rules to recognize string literals like `"abc"`
supporting simple escape sequences `\n`, `\\` and `\"`. We start
in the main rule by matching the opening delimiter:

[source,jl]
----
...
| '"' { StringBuilder buf = new StringBuilder();
        String s = string(buf); // Nested call
        return Token.STRING(s);
      }
----

When the opening `"` is matched, a buffer for the literal's contents
is allocated and passed to the nested rule `string` which will match
the literal and decode its contents into the buffer at the same time.

[source,jl]
----
private { String } rule string(StringBuilder buf) =
| '"'      { return buf.toString(); }
| '\\' 'n' { buf.append('\n'); continue string; }
| '\\' '"' { buf.append('"'); continue string; }
| '\\' '\\' { buf.append('\\'); continue string; }
| '\\'     { throw error("Invalid escape sequence"); }
| eof      { throw error("Unterminated string literal"); }
| orelse   { appendLexeme(buf); continue string; }
----

The nested rule returns the buffer's contents when encountering a
closing `"`. When it reads a valid escape sequence, it appends the
encoded character to the buffer. When it reads an invalid escape
sequence or encounters the unexpected end-of-input, it throws an
adequate error. Other characters are just appended to the buffer as
they are.

The <<Lexers_Adding_More_Entries,tutorial>> shows a more complete
example of this on the JSON format, in particular using another
nested rule dedicated to escape sequences.
====

.Example 2 (Nested comments)
====
Let us use nested rules to recognize nested multi-line `/* .. */`
comments. We will rely on a local field `depth` of type `int` to track
the current nesting level when analyzing the comments--this field must
have been declared in the prelude.  We start in the main rule by
matching the opening delimiter:

[source,jl]
----
...
| "/*" { depth = 0;
         ml_comments(); // Nested call
         continue main;
       }
----

When the opening `/*` is matched, the nesting depth is set to 0 and
the nested rule is called to start analyzing the comment's contents.

[source,jl]
----
private { void } rule ml_comments() =
| "*/"      { if (depth == 0) return;
              --depth; continue ml_comments;
            }
| "/*"      { ++depth; continue ml_comments; }
| newline   { newline(); continue ml_comments; }
| eof       { throw error("Unterminated comments"); }
| orelse    { continue ml_comments; }
| _         { continue ml_comments; }
----

The nested rule returns when encountering the closing `+*/+` while on
depth zero, and otherwise decrements the depth and continues running
the same rule. The depth is incremented when a nested `/*` is
encountered. The rule also takes care of tracking line breaks to
update the line and column numbers, and to throw an error should the
end-of-input be reached before the comments end.
////
It may seem more elegant to remove the instance field
and pass `depth` as a parameter to `ml_comments` instead, but
doing so would prevent the use of `continue` to optimize the
<<Lexers_Tail_Recursion_Ref,recursive calls>> to `ml_comments`.
A possible workaround would be to pass an object containing
a mutable integer
////
====

There is yet another possible advantage of using separate
dedicated entries for parts of a language, and it is to
factorize the rules for similar patterns which can be found
in different contexts. For instance, the escape sequences
available in string literals may be completely identical
to the escape sequences in character literals, and having
a dedicated entry for escape sequences can help factorize
the two in a single set of rules.

When using nested rules, one must be aware of the couple of
subtleties. First, it is possible write a recursive entry, or a group
of mutually recursive entries, in which case the user should be aware
of the possibility of stack overflow. This is discussed
<<Lexers_Tail_Recursion_Ref,below>>. Second, the internal state
of the lexing buffer is updated when performing nested calls,
and a number of the <<Lexers_Semantic_Actions_Ref,inherited methods>>
available during semantic actions do depend on this internal state.
For instance the following clause may observe the internal state
being overwritten by the nested call inside:

[source,jl]
----
| r { Position start = getLexemeStart(); <1>
      String lex = getLexeme();          <2>
      nested_rule();                      <3>
      Position start2 = getLexemeStart(); <4>
      String lex2 = getLexeme();          <5>
    }
----
<1> Retrieving the starting position of the lexeme matched by `r`.
<2> Retrieving the string matched by `r`.
<3> Performing some nested call, which overwrites the internal
    state of the lexer.
<4> Retrieving the starting position of the *last* lexeme
    matched in `nested_rule`.
<5> Retrieving the contents of the *last* lexeme matched
    in `nested_rule`.

If you find this situation surprising, consider that preventing it
would require Dolmen to keep a stack of all relevant information
across nested calls, which would be costly for the frequent cases when
this information is not needed. When it indeed is needed, it is
straightforward to just retrieve it before performing nested calls, as
is the case in the example above for `start` and `lex`. The
<<Lexers_Token_Locations,next section>> precisely covers how to deal
with managing token positions across nested rules.

[#Lexers_Token_Locations]
==== Managing Token Locations

_Coming soon_

[#Lexers_Tail_Recursion_Ref]
==== Tail-Recursion

When using <<Lexers_Nested_Rules,nested rules>> in a lexer
description, one should be wary of nested calls being so deep that the
lexical analysis could cause the Java program to excess its stack
space, leading to a
link:https://docs.oracle.com/javase/8/docs/api/java/lang/StackOverflowError.html[StackOverflowError]
being thrown. This is realistically only an issue with _recursive_
calls, i.e. when some rule is called from its own semantic actions,
either directly or indirectly through other rules.

For instance, consider a lexing rule `main()` with a clause for
skipping Python comments (assuming for the sake of conciseness that
the only line terminator is the line feed character):

[source,jl]
----
public { Token } rule main() =
...
| '#' [^'\n']* '\n' { newline(); return main(); }
...
----

This clause will consume the comment until (and including) the end of
the line, and continues analyzing the input stream with the same rule
by calling `main()` recursively. This recursive call to `main()` has
the peculiarity of being a
link:https://en.wikipedia.org/wiki/Tail_call[_tail-call_], i.e. that
this call is the very final action taken in this function. As such,
the call could be optimized so that the callee's frame can reuse the
space of the caller's frame, a process dubbed _tail-call
optimization_. Tail-call optimization is not limited to recursive
tail-calls, but it is an important feature of compilers for functional
languages where recursion is a key paradigm. Unfortunately, Java does
not have tail-call optimization in general, so code like the clause
above will use an amount of stack space that is proportional to the
largest number of consecutive comment lines `# ...` in the source.
This can be arbitrary large, and it is not rare to find very long
comment blocks in real code.

The standard countermeasure to prevent a recursive function from
blowing up the stack in the absence of tail-call optimization is to
rewrite the recursion as a regular loop. It is particularly
straightforward when the function was tail-recursive in the first
place. Dolmen provides this possibility by generating the code
for every lexer entry inside a `while` loop labelled with
the name of the rule itself. Therefore, the code for the entry
`main` will follow this structure:

[source,java]
----
public Token main() {
  main:
  while (true) {
    ... // Consumes input, find matching clause
    ... // Executes corresponding semantic action
  }
}
----

This layout allows using Java's `continue` statement to continue
executing the rule `main` without explicitly calling it recursively,
namely by rewriting our clause for Python comments as follows:

[source,jl]
----
public { Token } rule main() =
...
| '#' [^'\n']* '\n' { newline(); continue main; }
...
----

As the semantic action will simply jump back to the beginning of the
entry and start scanning again, this version of `main` uses a constant
amount of stack (barring issues in other clauses) and can accomodate
thousands of contiguous lines of comments.

The strategy of resorting to a `continue` statement has two
limitations.

. If the rule which is reentered using `continue` has parameters, it
  is not possible to change these parameters with `continue` whereas
  it would be possible with an explicit tail-call. For instance,
  consider a rule which recognizes nested `{ .. }`
  expressions--similarly to Dolmen's own rule for lexing semantic
  actions in lexer descriptions--and is parameterized by the current
  nesting `depth`, representing the number of `}` still to find:
+
[source,jl]
----
...
// Somewhere in another rule
| '{'  { return action(1); }
...

private { ... } rule action(int depth) =
| '{'  { return action(depth + 1); }
| '}'  { if (depth == 1) return ...;
         else return action(depth - 1);
       }
| _    { continue action; }
----
+
This snippet can use `continue` in the last clause, when simply
looking for more curly braces, but must use `return` in the first
two clauses in order to update the depth level accordingly. A
simple way to fix such a rule is to add an instance field
to the lexer's prelude and use it instead of passing a parameter
to  `action`. Another possibility is to define a mutable container
class for the rule's parameters, in our case a simple integer,
and pass such a container to the rule:
+
[source,jl]
----
{
  // In the prelude
  private static class DepthLevel {
    int val = 1;
  }
}

...

...
// Somewhere in another rule
| '{'  { return action(new DepthLevel()); }
...

public { ... } rule action(DepthLevel depth) =
| '{'  { depth.val++; continue action; }
| '}'  { if (depth.val == 1) return ...;
         depth.val--; continue action;
       }
| _    { continue action; }
----

. The second limitation with `continue` is that it can be used solely
  with direct recursion, and not in the case of several mutually
  recursive rules. This arguably rarely arises in practical cases.
  Consider two rules which recognize both `{ ... }`
  and `[ ... ]` blocks, delimited respectively by curly and square
  brackets. Suppose each kind of block can include the other kind
  of block, as long as things remain well-parenthesized. A natural
  description of these two rules could be:
+
[source,jl]
----
// Somewhere in main rule
| '{'  { return curlyBlock(); }
| '['  { return squareBlock(); }
...

private { ... } rule curlyBlock =
| '}'  { return; }
| '['  { return squareBlock(); }
| _    { continue curlyBlock; }

private { ... } rule squareBlock =
| ']'  { return; }
| '{'  { return curlyBlock(); }
| _    { continue squareBlock; }
----
+
This is simple enough, but the stack will not survive input which
contain long sequences like "[{[{[{[{[{[{[ .... ]}]}]}]}]}]}]}".  The
only way to force such rules to use constant stack space is to merge
the mutually recursive rules together into a single lexer entry. This
may be quite tedious in general depending on the rules' clauses, but
in this case this is rather pedestrian as the clauses from the two
rules do not really interfere. Nevertheless, we must take care of
tracking whether the last opened block was a curly or squared one,
and the overall number of all blocks that have been opened so far.
For that reason we introduce a local container class to represent
that state of the combined rule:
+
[source,jl]
----
{
  // In prelude
  private static Blocks {
    private Blocks(int depth, boolean inCurly) {
      this.depth = depth;
      this.inCurly = inCurly;
    }

    static Blocks newCurly() {
      return new Blocks(1, true);
    }    
    static Blocks newSquare() {
      return new Blocks(1, false);
    }

    int depth;
    boolean inCurly;
  }
}

// Somewhere in main rule
| '{'  { return blocks(Blocks.newCurly()); }
| '['  { return blocks(Blocks.newSquare()); }
...

private { ... } rule blocks(Blocks st) =
| '{'  { if (!st.inCurly) {
           st.depth++; st.inCurly = true;
         }
         continue blocks;
       }
| '}'  { if (st.inCurly) {
           st.depth--;
           if (st.depth == 0) return;
           st.inCurly = false;
         }
         continue blocks;
       }
| '['  { if (st.inCurly) {
           st.depth++; st.inCurly = false;
         }
         continue blocks;
       }
| ']'  { if (!st.inCurly) {
           st.depth--;
           if (st.depth == 0) return;
           st.inCurly = true;
         }
         continue blocks;
       }
| _    { continue blocks; } 
----
+
Now, do not worry if this transformation does not look that
"pedestrian". Arguably, one should only very rarely have a really good
reason to use a lexical analyzer to parse something like this.
It is possible nonetheless, and is worth showcasing for the sake
of completeness. Also, as a matter of style, a class such as `Blocks`
in the example above should probably be implemented in a companion
Java file and not in the lexer description's prelude. The prelude
should really only contain fields or local helper methods which
access the state of the underlying lexing engine.

<<<
// Include the language reference section
include::LEXER-syntax.adoc[leveloffset=+2]
