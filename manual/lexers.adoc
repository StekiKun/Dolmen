////
 This is the chapter on building lexical analyzers in the Dolmen documentation.
 Its master file is manual.adoc.
////

:jls8: https://docs.oracle.com/javase/specs/jls/se8
:jdk8: https://docs.oracle.com/javase/8/docs/api/java
:jdoc-base: {doc-base}/javadoc
:jdoc-dolmen: {jdoc-base}/org/stekikun/dolmen
:codegen: {jdoc-dolmen}/codegen
:lexbuf: {codegen}/LexBuffer.html
:debug: {jdoc-dolmen}/debug
:json-pdf: http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf

[#Lexers]
== Lexical Analyzers

////
 Plan:
   + Simple introductory example
     * Lexer structure
     * Main entry
     * Compiling and testing
     * Adding nested entries
     * Improving error reports
     * Debugging locations
   + Lexer entries
     * Regular expressions
     * Semantic actions
     * eof, _, orelse
     * options
     * CLI
   + Advanced concepts
     * nested rules
     * handling token locations
     * tail-recursion
   + Lexer description reference
////

In this chapter, we explain how to produce lexical analysers with
Dolmen by writing _Dolmen lexer descriptions_. We start by showing how
to write a simple lexer description for the JSON format before
presenting the various features of the lexer description format in
more detailed fashion. A complete reference for the lexer description
syntax can be found at <<Lexers_Syntax_Ref,the end of this chapter>>.

// The tutorial section
include::lexers-tutorial.adoc[]

<<<
=== Lexer Entries

The main building blocks of a Dolmen lexical analyzer are the lexer
_entries_. Dolmen will generate a single Java method per entry in the
resulting generated lexical analyzer. The syntax of a lexer entry is
as follows:

[listing, subs="{bnf-listing}"]
--
[[InJLEntry]]Entry :=
  (*public* | *private*)         // entry's visibility
  <<JLACTION,ACTION>>            // entry's return type
  *rule* <<JLIDENT,IDENT>>       // entry's name
  (<<JLACTION,ACTION>>)?         // entry's optional arguments
  *=* (*shortest*)?     // whether shortest or longest match rule is used
  <<JLClause,Clause>>+
--

An entry has a visibility (public or private), a return type, a name
and optional parameters: all of these will propagate directly to the
Java method generated from the entry. The remainder of the entry
is a sequence of _clauses_ which associate a regular expression
to some arbitrary Java _semantic action_:

[listing, subs="{bnf-listing}"]
--
[[InJLClause]]Clause :=
| *|* <<JLRegular,Regular>> <<JLACTION,ACTION>>
| *|* *orelse*  <<JLACTION,ACTION>>
--

TIP: Instead of a regular expression, a clause can be introduced
     with the special `orelse` keyword, whose meaning is explained
     in a <<Lexers_Wildcards,dedicated section below>>.

In essence, Dolmen lexers let you conveniently program methods which
consume an input sequence of characters, recognize some input patterns
described by regular expressions, and take on different actions
depending on what patterns have been matched. Whether you use these
actions to tokenize the input stream, to transform the input stream by
extracting, decoding, encoding or filtering parts of it, to count
occurrences of certain patterns or anything else, is completely
irrelevant to Dolmen. Dolmen's job is to take the high-level lexer
description and turn it into an efficient analyzer by taking care of
important details such as:

* managing input loading and buffering;

* compiling an entry's clauses into efficient code which matches
  input and select the adequate semantic action;

* keeping track of matched input positions, as well as those fragments
  corresponding to patterns captured via the `... as c` construct;

* assisting users by statically detecting and reporting common
  mistakes in lexer descriptions, such as useless clauses, etc.

When more than one clause in the entry matches the current input, the
default disambiguation rule is to pick the clause which yields the
_longest_ match. This behaviour can be changed for a given entry by
using the `*shortest*` keyword prior to the clauses: in this case,
priority is given to the clause yielding the shortest match. In any
case, when several clauses yield matches with the same length, the
clause which appears first in the lexer description will be selected.

The shortest-match rule is seldom used, at least for tokenizing
purposes. It can prove useful when writing text stream processors, in
particular when reading input from a network staream or from standard
input, as entries which use the shortest-match rule never need to
"look ahead" in the input stream and backtrack before selecting the
correct clause, unlike regular longest-match entries.

[#Lexers_Regular_Expressions]
==== Regular Expressions

In this section, we describe the Dolmen syntax for regular
expressions. Regular expressions appear as part of clauses in lexer
entries, but auxiliary regular expressions can also be defined
before the lexer entries:

[listing, subs="{bnf-listing}"]
--
[[InJLDefinition]]Definition :=
  <<JLIDENT,IDENT>> *=* <<JLRegular,Regular>> *;*
--

The identifier on the left-hand side in a regular expression
definition is the name which can be used to denote that regular
expression in subsequent definitions as well as in clauses. The
various regular expression constructs are summarized in
<<Lexers_Regexps_Table,this table>>, we detail each of them
in the following.

Literals::

The most basic form of regular expression are the _character_ and
_string_ literals. A character literal such as `'a'` or `'\040'` is a
regular expression which simply matches the given character.
Similarly, a string literal such as `"foo"` or `"\r\n"` is a regular
expression which matches the given string. The exact syntax for
character and string literals is detailed in the the
<<Lexers_Lexical_Conventions,lexical conventions>>.
+
[source,jl]
----
// Some literal regular expressions
space = ' ';
pub = "public";
----
+
Additionally, there are two "special" literals `+_+` and `eof`,
respectively called _wildcard_ and _end-of-input_. `eof` is
a meta-character with represents the fact that the end of
the input stream has been reached; in particular it does not
really match a given character, but simply succeeds when the
input stream has been exhausted. On the other hand, the
wildcard `+_+` will match any single character from the input,
and thus in particular is mutually exclusive with `eof`.
These special literals are discussed further
<<Lexers_Wildcards,below>>.

Character classes::
+
A _character class_ is a way of denoting a regular expression which
must only match a character in a subset of the whole character set. It
is introduced by square brackets `[...]` and can contain any non-empty
sequence of:
+
--
* single character literals, such as `'a'`, `'0'`;

* character ranges, such as `'a'-'z'`, `'0'-'9'`, denoting all the
  characters which belong to the specified range (inclusive on both
  ends);

* names of already defined regular expressions whose value corresponds
  to a character class (or can be reduced to a character class, such
  as a single character literal or the wildcard `+_+`).
--
+
The meaning of a character class can also be completely *inverted*
if it starts with a `^` character, in which case it matches any
character which does not belong to the specified set.
+
[source,jl]
----
// Some character classes
whitespace = ['\t' space '\f'];
alpha = ['a'-'z' 'A'-'Z'];
nonctrl = [^'\000'-'\037'];
----
The grammar for character classes is recalled in the
<<JLCharClass,syntax reference>>.

Character class difference::
The _difference operator_ `r # s` lets one define a regular
expression by taking the set of characters which match
the given class `r` but do not match the class `s`. The regular
expressions used as operands for `#` must reduce to character
classes or a lexical error will be reported by Dolmen.
+
[source,jl]
----
// Some character class differences
digit = ['0'-'9'];
nzdigit = digit # '0';
lowercase = alpha # ['a'-'z'];
ctrl = _ # nonctrl;
----

Repetitions::
+
Dolmen supports the following traditional postfix _repetition_
operators:
+
--
* `r?` matches zero or one occurrence of the regular expression `r`;
* `r+` matches one or more occurrences of the regular expression `r`;
* `r*` matches any number of occurrences of the regular expression
  `r`, including zero.
--
+
Besides these basic operators, it is also possible to specify any
finite number of repetitions using the following syntax where `n` and
`m` stand for literal decimal natural integers:
+
--
* `r<n>` matches exactly `n` occurrences of the regular expression `r`;
* `r<n,m>` matches anything between `n` and `m` (inclusive) occurrences
  of the regular expression `r`.
--
+
Note that `r<n>` is just syntactic shortcut for `r<n,n>`, and that
`r?` is completely equivalent to `r<0,1>`. There is no way to specify
an "infinite" upper bound, so `r+` and `r*` cannot be obtained with an
instance of `r<n,m>`. This is not a limitation in practice, since
something like any number of repetitions larger or equal to 3 can be
obtained by a combination such as `r<3>r*`.
+
[source,jl]
----
// Using repetition operators
decimal = digit+;
hexcode = [digit 'a'-'f' 'A'-'F']<4>;
word = ['_' alpha]+;
blanks = whitespace*;
----

Concatenation::
+
The concatenation of two regular expressions `r s` is a regular
expression which first matches `r` and then `s`. Concatenation is
_right-associative_, and has strictly lower precedence than every
operator we have seen so far. In particular, a string literal regular
expression such as `"foo"` is just syntactic sugar for the
concatenation `'f''o''o'` of its character literals. Similarly, a
repetition `r<4>` is just another, shorter, way of writing `r r r r`.
+
[source,jl]
----
// Using concatenation
float = decimal ('.' decimal)?;
string = '"' [^ ctrl '"']* '"';
r1 = 'a''b'+;  // ab......b
r2 = ('a''b')+;  // abab...ab
----
+
Note how parentheses can be used to disambiguate or regroup regular
expressions when natural precedence rules would not yield the intended
result.

Choice::
+
The _choice operator_ `r | s` matches anything that either regular
expression `r` or `s` matches. The choice operator is
_right-associative_, and has strictly lower precedence than every
operator we have seen so far, including concatenation. In particular,
the regular expression `r?` is equivalent to `r | ""`.
+
[source,jl]
----
// Using the choice operator
ident = alpha (alpha | digit | '_')*;
newline = '\r' | '\n' | "\r\n";
r3 = 'a'|'b''c';  // a or bc
r4 = ('a'|'b')'c';  // ac or bc
----

Capturing groups::
+
The _capture_ operator `r *as* c` matches exactly like the regular
expression `r` but when successful, it also saves the part of the
input stream which it matched. The matched contents are then made
available in the semantic action of the corresponding clause as a
local variable with name `c`. Captures are very convenient when
the semantic action would otherwise need to extract some interesting
part of the matched input, potentially at the cost of rescanning
some of it.
+
The type of a capture variable depends on the captured regular
expression, and on the overall clause's expression it appears in. It
can be either `String`, `char`, `Optional<String>` or
`Optional<Character>`. The `Optional` wrappers are introduced when the
overall regular expression can succeed without the captured regular
expression having matched at all, such as in `(r *as* c)?` or `(r1
*as* c) | r2`. Other than that, a capture variable is normally of type
`String`, unless it captures a regular expression which can only
statically match a _single_ character (e.g. a character literal, a
character class, a wildcard or an alternation of any of those), in
which case its type is that of a Java character.
+
====
The rationale behind the fact that the type of capture variables
adapts to the regular expression is twofold:

. Using a simpler type such as `char` in comparison to `String`
  enhances efficiency since no heap-allocation is required to capture
  a single character. Moreover, captured groups are always assigned
  before entering a clause's semantic action, even if it is never
  going to be used in the semantic action.

. Wrapping optional captured groups with `java.util.Optional` goes
  against raw efficiency instead, but for the very good reason of
  making sure that semantic actions always use captured variables in a
  way that is consistent with the associated clause. In particular,
  using `null` or default values instead would make it very easy
  for a developer to break a semantic action's intended behaviour
  by only slightly changing the associated regular expression.
  With our approach, such a breaking change will normally result
  in a type-checking error in the compiler.
====
+
Dolmen does not enforce that a capture variable appears only
once in a clause's regular expression. This allows for instance
patterns such as `(r1 *as* c) | (r2 *as* c)` where `c` can
then be handled in a uniform fashion in the semantic action.
The type of a capture variable which appears more than once
in a regular expression is simply the "simplest" type which
can accomodate all occurrences of the variable. For instance,
the capture variable can only have type `char` if all its
occurrences can have type `char`, and so on.
+
[source,jl]
.Examples of captures in action
----
// North-American style phone number, capturing groups
phone = (digit<3> as area) '-' (digit<3> as office) '-' (digit<4> as num);
// Markdown-style header, capturing the title
header = '#'+ blanks ([^ '\r' '\n']+ as title) newline;
// GCC #line directive, capturing filename and line number
linedir = "#line" blanks (digit+ as line) blanks (string as filename);
// Java Unicode escape sequence, capturing the code unit value
unicode = '\\' 'u'+ (hexcode as code);
----
+
Even when a capture variable is bound only once in a regular
expression, it is possible that it is part of a repeated expression,
for instance `(r *as* c)<2>`, and can therefore be matched several
times during a single overall match. In such a case, the variable will
be bound to the contents that correspond to the last match. Similarly,
if the same capture variable appears nested in a captured group, such
as in `(r0 (r1 *as* c) r2) *as* c`, only the outermost occurrence can
ever find its way into the semantic action. The innermost captures are
actually optimized away very early by Dolmen, and do not even
contribute to the choice of the type associated to the variable `c`.
+
Finally, there may be cases where there is not a unique way of
matching the input stream with some regular expression, and that the
different ways would yield different captured contents. For instance,
consider matching the string `aaaaa` with the regular expression
`('a'<2,3> *as* i) ('a'<2,3> *as* j)`: there are two correct ways in
which `i` and `j` could be assigned. In such a case, which one is
produced by the Dolmen-generated lexical analyzer is left unspecified.


The following table summarizes the different regular expression
constructs in order of *decreasing* precedence, i.e. operators
appearing first bind tighter than those appearing later down
the table.

[[Lexers_Regexps_Table]]
.Regular expression operators
[cols=4*,options="header"]
|===
| Name
| Syntax
| Meaning
| Associative

| Difference
| `r # s`
| Matches the characters in `r` which are not in `s`.
  Both `r` and `s` must reduce to character classes.
| No

| Option
| `r?`
| Matches like `r`, or the empty string.
| No

| Kleene star
| `r*`
| Matches any number of repetitions of `r`.
| No

| Kleene plus
| `r+`
| Matches one or more number of repetitions of `r`.
| No

| Repetition
| `r<n>`
| Matches exactly `n` repetitions of `r`.
| No

| Bounded repetition
| `r<n, m>`
| Matches between `n` and `m` (inclusive) repetitions of `r`.
| No

| Concatenation
| `r s`
| Matches `r` first and then `s`.
| Right-associative

| Choice
| `r \| s`
| Matches either `r` or `s`.
| Right-associative

| Capture
| `r as c`
| Matches like `r` but associates the matched part
  to a local variable named `c`.
| Left-associative

|===

[#Lexers_Semantic_Actions_Ref]
==== Semantic Actions

When a matching clause has been chosen by the lexing engine and the
corresponding prefix of the input consumed, the associated semantic
action is executed. The semantic action can be almost arbitrary Java
code, but must respect a few principles.

. The semantic action must be such that all paths are guaranteed
  to adequately exit the control-flow, which means that every
  execution path must end with either of the following:

  ** a `return` statement, compatible with the declared
    return type of the enclosing entry;
  
  ** the throwing of an uncaught exception (preferably a
     link:{codegen}/LexBuffer.LexicalError.html[`LexicalError`]
     exception);

  ** a Java `continue lbl` statement to restart the current lexer
     entry `lbl` without actually calling the Java method implementing
     the entry (cf. <<Lexers_Tail_Recursion_Ref,tail-recursion>>);
     this works because Dolmen always generates the code of a lexer
     entry in a loop with a
     {jls8}/html/jls-14.html#jls-LabeledStatement[labeled statement]
     whose label is exactly the name of the entry.

+
When this principle is not respected, the generated code may
contain errors or warnings, and the behaviour of the generated
lexical analyzer is not predictable.

. The generated lexer is a subclass of the
link:{codegen}/LexBuffer.html[`LexBuffer`] class from the Dolmen
runtime, and as such has access to a number of fields and methods
inherited from `LexBuffer`. Some of these fields and methods are
provided primarily for use by user code in semantic actions, whereas
others deal with the internal functionality of the lexing engine and
should *not* be called or tampered with in semantic actions. The
reader may wonder why these fields and methods which are reserved for
internal use are declared as `protected`; they cannot be made
`private` in `LexBuffer` because they must be accessed from the
code of the subclass generated by Dolmen itself.
+

In order to document which protected members are intended for user
code and which are off limits, the
link:{codegen}/DolmenInternal.html[`DolmenInternal`] annotation has
been introduced and is used to annotate every member which is declared
as `protected` but should not be accessed from user code (i.e. either
from semantic actions or from the prelude and postlude).  As a
refinement, a field can be annotated with `@DolmenInternal(read = true)`
in which case access to the field from user code is tolerated as long
as the value is not modified. In this context, "modifying" a field
encompasses both modifying the field itself, and in the case the field
is a reference object, modifying the actual value of the referred object
without changing the reference itself. For instance, the following
declaration is possible:
+
[source,java]
----
@DolmenInternal(read = true)
protected final Set<Integer> numbers;
----
+
It signifies that `numbers` can be read from user code--the field
itself cannot be modified as it is final anyway--but only methods
which do not change the contents of `numbers` should be allowed.  As
the Java language lacks a proper notion of `const`-ness, it cannot be
expressed in method signatures whether they observationally change
their receiver's state or not so one must rely on documentation and
common sense in practice.
+
When internal members are used in semantic actions, the lexical
analyzer may behave unexpectedly as this can break some of the
internal invariants it relies on. Even if things seem to work fine,
such code can be broken by a future Dolmen update.

We conclude this section on semantic actions with an exhaustive
presentation of the fields and methods from
link:{codegen}/LexBuffer.html[`LexBuffer`] which are available for use
in semantic actions. One can always refer to the Javadoc associated
to these members, either via the link:{codegen}/LexBuffer.html[HTML pages]
or using your IDE when accessing the Dolmen runtime.

:field-pro: image:field_protected_obj.png[title="protected field"]
:meth-pro: image:methpro_obj.png[title="protected method"]
:meth-pub: image:methpub_obj.png[title="public method"]

Input position::
  The following members available in semantic actions relate
  to the management of the input position.      
+
--
{field-pro} `String filename`::
            This field contains the _name_ of the current input
            being analyzed by the lexing engine. Despite the field's name,
            this name need not be an actual filename, it is only used in
            positions and error reporting. +
            It is not final as it may be
            changed for instance when handling a GCC-like `#line`
            directive.
{field-pro} `Position startLoc`::
            This field contains the starting position of the current lexeme. +
            It can be modified before returning from a semantic action, e.g.
            for adjusting <<Lexers_Token_Locations,token locations>> when
            the lexical analyzer is used by a parser.
{field-pro} `Position curLoc`::
            This field contains the ending position of the current lexeme,
            i.e. the position of the first character that _follows_ the
            lexeme. +
            It can be modified before returning from a semantic action, e.g.
            for adjusting <<Lexers_Token_Locations,token locations>> when
            the lexical analyzer is used by a parser.
{meth-pro} `void newline()`::
           Modifies the internal line and column counter to account
           for the consumption of a line terminator. This needs to be
           called in semantic actions accordingly if one wants to keep
           correct track of lines and columns.                 
{meth-pro} `<T> T saveStart(Supplier<T> s)`::
           This convenience method calls the given supplier `s`, returning
           the same value as ̀s`, but saving the current value of `startLoc`
           before the call and restoring it afterwards. This is used when
           <<Lexers_Nested_Rules,nested rules>> are used to parse a token
           and the token location needs to be adjusted.
{meth-pro} `<T> T savePosition(Supplier<T> s, Position start)`::
           This convenience method calls the given supplier `s`, returning
           the same value as ̀s`, but setting the value of `startLoc` to
           `start` afterwards. This is used when
           <<Lexers_Nested_Rules,nested rules>> are used to parse a token
           and the token location needs to be adjusted.
{meth-pro} `void saveStart(Runnable r)`::
           This convenience method calls the given runnable `r`, saving
           the current value of `startLoc` before the call and restoring
           it afterwards. This is used when
           <<Lexers_Nested_Rules,nested rules>> are used to parse a token
           and the token location needs to be adjusted.
{meth-pro} `void savePosition(Runnable r, Position start)`::
           This convenience method calls the given runnable `r` and sets
           the value of `startLoc` to `start` afterwards. This is used when
           <<Lexers_Nested_Rules,nested rules>> are used to parse a token
           and the token location needs to be adjusted.
--

Lexeme::
  The following members available in semantic actions relate
  to the retrieval of the last matched lexeme. They all refer
  to the part of the input which was last matched by a lexer
  clause; in the case where there have been calls to a 
  <<Lexers_Nested_Rules,nested rule>>, these methods will return
  information relevant to the last matched clause, and not to
  the lexeme which was matched by the clause of the _current_
  semantic action.

+
--
{meth-pub} `String getLexeme()`::
           Returns the last matched lexeme.
{meth-pub} `Position getLexemeStart()`::
           Returns the starting position of the last matched lexeme.
{meth-pub} `Position getLexemeEnd()`::
           Returns the end position of the last matched lexeme. By convention,
           this is the position of the first character that _follows_ the
           lexeme.
{meth-pub} `int getLexemeLength()`::
           Returns the length of the last matched lexeme.
{meth-pub} `char getLexemeChar(int idx)`::
           Returns the character at index `idx` in the last matched lexeme.
           The index is 0-based, and must be between 0 (inclusive) and
           `getLexemeLength()` (exclusive).
{meth-pub} `CharSequence getLexemeChars()`::
           Returns the contents of the last matched lexeme as a character
           sequence. The character sequence refers to the current internal
           state of the lexing engine and becomes unspecified after 
           <<Lexers_Nested_Rules,nested rule calls>>, even if it was retrieved
           before any nested call.
{meth-pub} `void appendLexeme(StringBuilder buf)`::
           Appends the contents of the last matched lexeme to the
           given buffer `buf`.
--

Input management::
  The following members available in semantic actions
  relate to the management of the input stream during a lexical
  analysis. The lexing engine supports switching input streams
  in the middle of an analysis, and can manage an _input stack_
  to let users implement inclusion directives like GCC's `#include`
  or TeX's `\input`.
+
--
{meth-pro} `void changeInput(String fn, Reader reader)`::
           This closes the current input stream and switches to the new
           input specified by `reader` and whose display name will be `fn`.
{meth-pro} `void pushInput(String fn, Reader reader)`::
           This pushes the current input stream on the input stack and
           start consuming the given `reader` instead. The stacked
           input stream can be resumed with `popInput()`.
{meth-pro} `boolean hasMoreInput()`::
           This returns whether the current stream is the last one
           remaining in the input stack. In other words, only when
           this method returns `true` can `popInput()` be used.
{meth-pro} `void popInput()`::
           This closes the current stream and resumes the last one
           which was pushed to the stack. Whether the stack is
           empty or not can be checked beforehand with `hasMoreInput`.
--

Error management::
  The following members available in semantic actions
  relate to error management during a lexical analysis.
+
--
{meth-pro} `LexicalError error(String msg)`::
           Returns a link:{codegen}/LexBuffer.LexicalError.html[`LexicalError`] exception
           with the given message string and reported at the current lexer position.
           This is the preferred way of reporting lexical errors in semantic actions.
           Note that unlike the other methods here, `error` is not final: it can be
           overriden so that one can for instance customize the way the error message
           is built and presented. In particular it could be internationalized if need be.
{meth-pro} `char peekNextChar()`::
           Returns the character which directly follows the last matched lexeme in
           the input stream, or 0xFFFF if end-of-input has been reached. This can be
           used to produce better error messages, or in some cases where a semantic
           action needs to look ahead at the stream without consuming it to decide
           what should be done.
{meth-pro} `int peekNextChars(char[] buf)`::
           Copies the characters which directly follow the last matched lexeme in
           the input stream to `buf`, and returns the number of characters actually
           copied. This can be used to produce better error messages, or in some cases
           where a semantic action needs to look ahead at the stream without consuming
           it to decide what should be done.
--

[#Lexers_Wildcards]
==== Understanding `eof`, `_` and `orelse`

In this section, we discuss the meaning of the special regular
expressions `eof` and `_`, which can sometimes be confusing, as well
as the special `orelse` keyword which can be used to introduce a
"catch-everything-else" clause in a lexer entry.

End-of-input::

As explained <<Lexers_Regular_Expressions,above>>, `eof` is a special
regular expression literal which matches when the input stream has
been exhausted. It actually represents the single meta-character
`\uFFFF`, which is used internally to represent the end-of-input and
is not a valid escaping sequence in character or string literals.
Even if `eof` is internally encoded as a character, it does not behave
as a character:

* matching `eof` does not _consume_ any character from the input
  stream, it simply checks that the end of the input stream has
  indeed been reached; therefore it always matches an empty
  input string, whereas regular characters always match an input
  of size 1;
* because it does not consume anything, `eof` has some rather
  exotic properties when it comes to regular expressions;
  for instance, `eof` and `eof+` are completely equivalent;
* `eof` is not considered as a regular expression which reduces to a
  character class; hence it is not possible to use `eof` in a
  character set difference operation `.. # ..`, or inside a character
  class `[.. eof ..]`;
* finally, even though `eof` stands for the end of the input stream,
  any regular expression which matches the empty string can still
  match after `eof`, e.g. `eof ""`, `eof ("" as c)` or even `eof
  ("foo"?)` will all successfully match the empty string `""`.

+
All in all, writing complex regular expressions using `eof` should be
really uncommon. As a rule of thumb, `eof` is usually best used all by
itself in a clause, as the end-of-input normally calls for some ad hoc
action, in particular in terms of <<Lexers_Error_Reports,error
reporting>>.

Wildcard::

The wildcard `+_+` is a regular expression which can match any valid
character. It is actually equivalent to the character class
`['\u0000' - '\uFFFE']`, and in particular *excludes* the end-of-input
meta-character. Therefore, `_` will behave in an expected way,
consuming exactly one character of the input stream, much like `.` in
many common regular expression engines.
+
What sometimes causes confusion is that `+_+` is not sufficient when
trying to write a default clause to catch any unexpected character for
instance:
+
[source,jl]
----
| _ as c { throw error("Unexpected character: " + c); }
----
+
This will catch any actual unexpected character but will still leave
`eof` unhandled--and if so, this will be reported by Dolmen as a
warning for a potentially incomplete lexer entry. It is possible to
match `(_ | eof) as c` or `(_ as c) | eof` of course, but whereas `c`
in the clause above had type `char`, `c` in the latter examples would
be an `Optional<Character>` which means the semantic action will not
handle both cases in a uniform manner anyway. It is usually simpler to
just have another clause dedicated to `eof`.
+
Note that for the same reason that `+_+` only matches actual non
end-of-input characters, the character class complement operation
`[^...]` only contains characters in the 0x0000-0xFFFE range. In
other words, the complement `[^ cset ]` of some character set
is actually equivalent to `_ # [cset]`.

Clause `orelse`::

The keyword `orelse` can be used in place of a regular expression to
introduce a special form of clause:
+
[source, jl]
----
| orelse  { ... }
----
+
The meaning of the `orelse` pattern depends on the context in which it
is used, namely of the other clauses declared before in the same lexer
entry. Its purpose is to match the longest possible prefix of input
which could not possibly match any of the other clauses above.
+
More precisely, suppose `orelse` is preceded by _n_ clauses with
respective regular expressions `r1`, ... `rn`; the regular expression
that `orelse` will denote is computed as follows:
+
--
. For each `ri`, compute the character set `Ci` of the characters
  which can start a match of `ri`. This character set can easily
  be computed by induction on the regular expression; for instance,
  it is the first character of the string for a string literal,
  the union of the first characters of `s` and `t` for a choice
  expression `s | t`, and so on.

. Compute the union `C` of all the character sets `Ci`, i.e. the set
  of all the characters which can start a match of any of the `ri`.

. Then, `orelse` will represent the regular expression `[^C]+`,
  i.e. it will match any non-empty number of characters which
  cannot start any of the `ri`.
--
+
An `orelse` clause is useful when one is only interested in
recognizing certain patterns, but can pass over the rest of the
input without having to interpret it. Consider for instance
a lexer entry `mlcomment` which is entered when starting
a (possibly multi-line) comment `/* ... */` and which must
match until the end of the comment:
+
[source,jl]
.An entry for multi-line comments
----
private { void } rule mlcomment =
| "*/"    { return; }
| newline { newline(); continue mlcomment; }
| eof     { throw error("Unterminated comment"); }
| ...
----
+
where `newline` is defined as `('\r' | '\n' | "\r\n")`.  As it is, the
entry recognizes the closing comment delimiter and also handles line
breaks and end-of-input gracefully. It still needs to be able to pass
everything else that the comment can contains. One correct way to do
that is to use a wildcard to catch everything else:
+
[source,jl]
----
...
| _     { continue mlcomment; }
----
+
In particular this works because by the longest-match rule, it will not
match a `*` character if it happens to be followed by a `\`. Unfortunately,
reentering the rule at every character is somewhat inefficient in comparison
to matching largest parts of the comment at once; indeed, every time a
rule is entered or reentered, the generated lexical analyzer performs some
minor bookkeeping regarding the current lexeme's positions. To pass as much
of the comment's contents as we possibly can in one go, we can use an ad-hoc
regular expression like the following:
+
[source,jl]
----
...
| [^'*' '\r' '\n']+     { continue mlcomment; }
----
+
The issue with this is twofold:
+
--
. On one hand, it is fragile as it relies on correctly listing
  all characters which are of interest to the other clauses in
  the rule. Should we forget one of them, the Kleene `+` operator
  and the longest-match rule will conspire to make sure our entry
  does not work as intended. Also, this must be maintained when
  adding new patterns to the rule.
. On the other hand, it still does not cover the case of `*`
  characters which are not followed by a `/`. One could of course
  fix the regular expression like this:
+
[source,jl]
----
...
| ([^'*' '\r' '\n']+ | '*'+[^'/' '*'])+  { continue mlcomment; }
----
+
but this would definitely make the first issue even more pregnant
(Hint: incidentally, this regular expression is still not good
enough!).
--
+
Reliability and reviewability are design principles in Dolmen, and the
`orelse` clause was brought up precisely to bring an elegant solution
for such a case. Using `orelse` here will be equivalent to `[^'*' '\r'
'\n']+` but in a much less error-prone way:
+
[source,jl]
----
private { void } rule mlcomment =
| "*/"    { return; }
| newline { newline(); continue mlcomment; }
| eof     { throw error("Unterminated comment"); }
| '*'     { continue mlcomment; }
| orelse  { continue mlcomment; }
----
+
Of course, lonely `*` characters still need to be singled out but this
is reasonably simple. One can even switch the last two clauses around
and use a wildcard:
+
[source,jl]
----
private { void } rule mlcomment =
...
| orelse  { continue mlcomment; }
| _       { continue mlcomment; }
----
+
Writing the entry this way will make maintenance really
straightforward. For instance, suppose we want to start
allowing nested `/* ... */` comments, this can be achieved
by keeping track of the current comment "depth" in a local
field and add a clause to handle opening delimiters:
+
[source,jl]
----
private { void } rule mlcomment =
| "*/"    { if (--depth == 0) return; continue mlcomment; }
| "/*"    { ++depth; continue mlcomment; }
| newline { newline(); continue mlcomment; }
| eof     { throw error("Unterminated comment"); }
| orelse  { continue mlcomment; }
| _       { continue mlcomment; }
----
+
The benefit of using `orelse` and a wildcard instead of complex
hand-crafted expressions is immediately apparent, as this is all that
we have to do to handle nested comments in our entry. `orelse` will
now take care of not consuming `/` characters, whereas the wildcard
will deal with `/` characters that are not followed by a `*`.
+
Note that as in the example above, the `orelse` clause need not
be the last one in the entry, it will simply depend on the clauses
defined above. It is even syntactically possible to have more than
one `orelse` clause in an entry, although all such extra clauses
will be shadowed by the first one by construction.

[#Lexers_Options]
==== Lexer Options

The lexical analyzer generator accepts some options which can
be specified right at the top of a lexer description, with
the following syntax:

[source,jl]
.Syntax for configuration options
--
[key1 = "value1"]
[key2 = "value2"]
...
--

A configuration option given as a key-value pair where the
key identifies the option being set, and the value is a
string literal. Unlike string literals in regular expressions,
option values can be multi-line string literals.

In the following, we list the different configuration options
available in lexer descriptions and document their effect.

class_annnotations::

The `class_annotations` option lets one specify arbitrary annotations
that will be added on the class generated by the lexical
analyzer. Indeed, a lexer description will result in the generation of
a single public class extending the link:{codegen}/LexBuffer.html[base
lexing buffer class] from the Dolmen runtime.
+
This can be useful in particular to add particular instances of
`@SuppressWarnings(...)` annotations. Dolmen tries very hard to
generate code without warnings--not counting warnings from the
semantic actions of course, at least for a reasonable default
configuration of the Java compiler. When working in a project with
stronger requirements, generated classes may have warnings and the
`class_annotations` option can be used to explicitly ignore these
warnings. For instance, to suppress warnings on missing Java
documentation in the generated lexer, one can use:
+
[source,jl]
----
[class_annotations = "@SuppressWarnings(\"javadoc\")"]
----
+
When referencing an annotation from another package, make sure it is
fully qualified or add the corresponding import to the imports section
of the lexer description, so that the annotation can be resolved in
the generated Java compilation unit. For instance, when working in a
project which uses the
link:https://help.eclipse.org/2019-12/index.jsp?topic=%2Forg.eclipse.jdt.doc.user%2Ftasks%2Ftask-using_null_annotations.htm[Eclipse
JDT null analysis], one may need something like this:
+
[source,jl]
----
[class_annotations = "@org.eclipse.jdt.annotation.NonNullByDefault"]
----
+
as Dolmen is implemented with the full null analysis enabled and uses
a non-null type default policy.


[#Lexers_CLI]
==== Command Line Interface

The Dolmen runtime is an executable Java archive which can be used as
a command-line tool to generate a lexical analyzer from a lexer
description. Another way of using Dolmen is via the companion
https://dolmenplugin.stekikun.org[Eclipse plug-in]; nonetheless, the
command line is useful in a variety of scenarios like building
scripts, Makefile, continuous integration, etc.

The command line interface can be invoked as follows:

[source,console]
----
$ java -jar Dolmen_1.0.0.jar -l ..options.. source
----

where `source` is the name of the source lexer description. The `-l`
option instructs Dolmen that the source is a lexer description and that
a lexical analyzer should be generated. The `-l` flag can be omitted
if the source file has the `.jl` extension. The other available
command line options are detailed in the following table.

.Command line options
[cols=4*,options="header"]
|===
| Name
| Type
| Default
| Description

| `-h/--help`
| Flag
| No
| Displays help about command line usage.

| `-q/--quiet`
| Flag
| No
| Quiet mode: suppresses all output except for error report.

| `-l/--lexer`
| Flag
| No
| Generates a lexical analyzer from the source file.

| `-o/--output`
| String
| Working directory
| Output directory for the generated class.

| `-c/--class`
| String
| Name of the source file
| Name of the generated class.

| `-p/--package`
| String
| *Required*
| Name of the package for the generated class.

| `-r/--reports`
| String
| Name of the source file + `.reports`
| No
| File where potential problems should be reported.

|===

String options must follow the option name directly, e.g.  `-o mydir
--class Foo` instructs Dolmen to generate the class `Foo.java` in the
`mydir` directory.  Flag values in their short form can be grouped
together. For instance, `-lq` instructs Dolmen to generate a lexical
analyzer in quiet mode. **Note that the `--package/-p` option is
required:** the generated class will be declared in the given package,
and Dolmen does not support generating classes in the _default_
package.

.Example 1
====
The following command will generate a lexical analyzer in
`src/foo/bar/MyLexer.java`, in the package `foo.bar`.
Only errors will be reported on the standard output, other
reports will be in `MyLexer.jl.reports`.

[source,console]
----
$ java -jar Dolmen_1.0.0.jar -q -p foo.bar -o src/foo/bar MyLexer.jl
----
====

.Example 2
====
The following command will generate a lexical analyzer in
`src/foo/bar/MyLexer.java`, in the package `foo.bar`.
The `-l` flag is required as Dolmen cannot recognize the
extension of the source description. +
Dolmen will output information on the different phases
of the lexical analyzer generation, including any errors.
Other problem reports will be written to `lexer.reports`.

[source,console]
----
$ java -jar Dolmen_1.0.0.jar -l -p foo.bar --reports lexer.reports src/foo/bar/MyLexer.lexer
----
====

=== Advanced Concepts

_Coming soon_

[#Lexers_Nested_Rules]
==== Nested Rules

_Coming soon_

[#Lexers_Token_Locations]
==== Managing Token Locations

_Coming soon_

[#Lexers_Tail_Recursion_Ref]
==== Tail-Recursion

_Coming soon_

<<<
// Include the language reference section
include::LEXER-syntax.adoc[leveloffset=+2]
