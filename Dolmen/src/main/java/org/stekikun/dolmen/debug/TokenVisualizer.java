package org.stekikun.dolmen.debug;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.Reader;
import java.io.StringReader;
import java.io.Writer;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.function.BiFunction;
import java.util.function.Function;

import org.eclipse.jdt.annotation.NonNull;
import org.eclipse.jdt.annotation.Nullable;
import org.stekikun.dolmen.codegen.CodeBuilder;
import org.stekikun.dolmen.codegen.LexBuffer;
import org.stekikun.dolmen.codegen.LexBuffer.LexicalError;
import org.stekikun.dolmen.common.Nulls;

/**
 * This class provides utility methods to help debug lexical analyzers
 * which have been generated with Dolmen. They provide a way to show
 * the exact result of tokenizing some input as a decorated HTML page
 * showing the various tokens and their associated locations on top
 * of the original input contents.
 * <p>
 * These methods are particularly useful for conveniently checking
 * the behaviour of a lexical analyzer, in particular in terms of
 * the positions associated to each token. These positions are often
 * used in parsers built on top of the lexical analyzer, and are hard
 * to thoroughly check and debug. Visualizing all tokens at once <i>in situ</i>
 * is a great help for that purpose. 
 * If you are interested in recording the precise results of a lexical analysis,
 * for instance for the purpose of continuous integration, consider using
 * the {@link Tokenizer} debugging feature.
 * <p>
 * <i><b>Beware:</b> the token visualization feature does not support
 * lexical analyzers which manipulate the token locations ({@link LexBuffer#curLoc}
 * and {@link LexBuffer#startLoc}) in ways that are not monotonic. This is for
 * example the case with lexical analyzers supporting GCC-like {@code #line}
 * or {@code #include} directives, or any analyzer which will move to another
 * input stream midway through the analysis in general. To debug such lexers,
 * consider using the less fancy {@link Tokenizer} interface.
 * </i>
 * 
 * @see #visualize(LexerInterface, String, Reader, Writer)
 * @see #string(LexerInterface, String, Writer)
 * @see #file(LexerInterface, String, String)
 * 
 * @author Stéphane Lescuyer
 */
public final class TokenVisualizer {
	
	/**
	 * A static list of colors which are used to highlight the various categories of tokens.
	 * <p>
	 * These colors have been picked arbitrarily with a couple criteria in mind, such as
	 * light enough for a black font to be readable over it, and so that the colors would contrast
	 * enough with each other.
	 */
	private static final @NonNull String COLORS[] = {
		"LightSkyBlue", "MediumSpringGreen", "Khaki", "LightPink", "LightGray", "LemonChiffon", "Plum",
		"Aquamarine", "DodgerBlue", "SandyBrown", "Beige", "HotPink", "Lavender", "LightGreen", "Tan", "Thistle",
		"Tomato", "LightCyan", "MediumTurquoise", "DarkKhaki", "PeachPuff", "Orchid", "LightSteelBlue",
		"LightSalmon", "YellowGreen", "PaleGoldenRod", "RosyBrown", "LightBlue", "MistyRose", "Orange",
		"DarkSeaGreen", "LightCoral", "Turquoise", "NavajoWhite", "LimeGreen", "DarkSalmon"
	};
	
	/**
	 * This interface acts as a generic proxy to using a Dolmen-generated
	 * lexer in the static debugging functions provided in {@link TokenVisualizer}.
	 * It must provide a way to {@link #makeLexer(String, Reader) create a lexer},
	 * an {@link #entry(LexBuffer) entry point} to use to tokenize input,
	 * and a {@link #halt(Object) halting condition} used to stop the tokenization
	 * (typically, recognizing an <i>end-of-input</i> special token). Additionally,
	 * it must provide a way to sort tokens in various {@link #category(Object) categories};
	 * tokens in the same category will be highlighted in a similar fashion. The categories
	 * can be anything, typical choices could be:
	 * <ul>
	 * <li> using {@link Class} as the category type and {@link #getClass()} as the
	 * 		category function, to differentiate between various kinds of valued
	 * 		tokens on one side, and constant tokens on the other side;
	 * <li> using the token class itself as the category type and {@link Function#identity()}
	 * 	    as the category function, to differentiate all distinct tokens;
	 * <li> using the token enum kind generated by a Dolmen parser as the category
	 * 		type, and the corresponding {@code getKind()} method to categorize
	 * 		tokens based on what terminal symbol they correspond to.
	 * </ul>
	 * 
	 * <p>
	 * For convenience, a static factory 
	 * {@link LexerInterface#of(BiFunction, Function, Function, Object)} 
	 * is provided, so that if {@code MyLexer} has been generated by Dolmen with
	 * a {@code main} entry point and uses some {@code Token.EOF} token for the
	 * end-of-input, one can simply use:
	 * <pre>
	 *   LexerInterface.of(MyLexer::new, MyLexer::main, Token::getClass, Token.EOF)
	 * </pre>
	 * to build a suitable tokenizing interface for that lexical analyzer which
	 * simply differentiates tokens based on their Java class.
	 * 
	 * @author Stéphane Lescuyer
	 *
	 * @param <L>	the type of generated lexing buffer to analyze
	 * @param <T>	the type of tokens produced
	 * @param <Cat> a category type for tokens
	 */
	public static interface LexerInterface<L extends LexBuffer, T, Cat> 
		extends Tokenizer.LexerInterface<L, T> {
		/**
		 * Tokens with the same category
		 * 
		 * @param token
		 * @return the category associated to {@code token}
		 */
		Cat category(T token);
		
		/**
		 * Typical usage of this method when {@code MyLexer} has been generated
		 * by Dolmen with a {@code main} entry point and uses some {@code Token.EOF}
		 * token for the end-of-input:
		 * <pre>
		 *   LexerInterface.of(MyLexer::new, MyLexer::main, Token::getClass, Token.EOF)
		 * </pre>
		 * 
		 * @param makeLexer
		 * @param entry
		 * @param categorizer
		 * @param eofToken
		 * @return a token visualization interface which builds lexical buffers
		 * 	using {@code makeLexer}, uses the entry point described by
		 *  {@code entry}, highlights tokens depending on their category as
		 *  returned by the {@code categorizer}, and stops as soon as the
		 *  token {@code eofToken} is encountered.
		 */
		public static <L extends LexBuffer, T, Cat>
		LexerInterface<L, T, Cat> of(
			BiFunction<String, Reader, L> makeLexer,
			Function<L, T> entry, Function<T, Cat> categorizer, T eofToken) {
			
			return new LexerInterface<L, T, Cat>() {
				@SuppressWarnings("null")
				@Override
				public L makeLexer(String inputName, Reader reader) {
					return makeLexer.apply(inputName, reader);
				}

				@Override
				public T entry(L lexbuf) throws LexicalError {
					return entry.apply(lexbuf);
				}

				@Override
				public boolean halt(T token) {
					return Objects.equals(eofToken, token);
				}

				@Override
				public Cat category(T token) {
					return categorizer.apply(token);
				}
			};
		}
	}

	/**
	 * The base class of recorded locations to highlight.
	 * 
	 * @author Stéphane Lescuyer
	 *
	 * @param <T>
	 */
	private static abstract class Location<T> {
		final LexBuffer.Position start;
		
		Location(LexBuffer.Position start) {
			this.start = start;
		}
		
		/**
		 * @return the end offset of the location, i.e.
		 * 	the absolute offset of the first position after the location
		 */
		abstract int end();
		
		/**
		 * @return the user-friendly description of this location 
		 */
		abstract String description();
		
		/**
		 * @param loc
		 * @return {@code true} if and only if this location
		 * 	intersects with {@code loc}
		 */
		@SuppressWarnings("unused")
		final boolean intersects(Location<T> loc) {
			if (end() <= loc.start.offset) return false;
			if (loc.end() <= start.offset) return false;
			return true;
		}
	}

	/**
	 * Records the location of some token returned by a lexical analyzer.
	 * <p>
	 * It contains both the start and end position of the token in the
	 * input stream, as well as the token itself and the attached color
	 * for highlighting purposes.
	 * 
	 * @author Stéphane Lescuyer
	 *
	 * @param <T>
	 */
	private static final class Token<T> extends Location<T> {
		final LexBuffer.Position end;
		final T token;
		@SuppressWarnings("unused")
		final @Nullable String color;
		
		Token(LexBuffer.Position start, LexBuffer.Position end, T token, @Nullable String color) {
			super(start);
			this.end = end;
			this.token = token;
			this.color = color;
		}
		
		@Override
		int end() {
			return end.offset;
		}
		
		@Override
		String description() {
			StringBuilder buf = new StringBuilder(Objects.toString(token));
			buf.append("\n");
			buf.append("(at line ").append(start.line)
				.append(", column ").append(start.column()).append(")");
			return buf.toString();
		}
	}
	
	/**
	 * A special type of location to record lexical errors.
	 * <p>
	 * It spans the interval between the position where the lexical 
	 * error occurred up to the last position matched by the lexer.
	 * In case this interval is empty (e.g. for an empty token error)
	 * it reports on the first character at this position.
	 * 
	 * @author Stéphane Lescuyer
	 *
	 * @param <T>
	 */
	private static final class LexError<T> extends Location<T> {
		final LexicalError e;
		final LexBuffer.Position end;
		
		/**
		 * {@code e} must have a non-null position.
		 * 
		 * @param e
		 */
		LexError(LexicalError e, LexBuffer.Position end) {
			// We are only building locations for lexical errors which
			// have a non-null position
			super(Nulls.ok(e.pos));
			this.e = e;
			this.end = end;
		}
		
		@Override
		int end() {
			return end.offset == start.offset ? 
					start.offset + 1 : end.offset;
		}
		
		@Override
		String description() {
			return "Lexical error: " + e.getMessage();
		}
	}

	/**
	 * An instance of this class will accumulate all the encountered tokens
	 * and relevant locations to highlight during the tokenization of the
	 * input to visualize.
	 * <p>
	 * In particular, it will associate colors to various categories of tokens
	 * as they are encountered and record all the relevant information to be
	 * able to emit the HTML output later on.
	 * 
	 * <i>NB: If we need another back-end, such as a dedicated Eclipse view,
	 * this class will have everything we need as it does not depend on a
	 * particular back-end.</i>
	 * 
	 * @author Stéphane Lescuyer
	 *
	 * @param <L>
	 * @param <T>
	 * @param <Cat>
	 */
	private static final class Acc<L extends LexBuffer, T, Cat> {
		final L lexbuf;
		final LexerInterface<L, T, Cat> lexer;
		
		// The next token category "type" to use
		int nextType;
		// The next token category "color" to use
		int nextColor;
		// A map associating already encountered categories to their color
		final Map<Cat, String> tokenColors;
		// A map associating already encountered categories to their index
		final Map<Cat, Integer> tokenTypes;
		// An ordered list of all registered locations so far
		final List<Location<T>> locations;
		// The number of tokens encountered so far
		int numTokens;
		
		Acc(L lexbuf, LexerInterface<L, T, Cat> lexer) {
			this.lexbuf = lexbuf;
			this.lexer = lexer;
			
			this.nextType = 0;
			this.nextColor = 0;
			this.tokenColors = new HashMap<>();
			this.tokenTypes = new HashMap<>();
			this.locations = new ArrayList<>();
			this.numTokens = 0;
		}

		/**
		 * @param token
		 * @return an index for this token's category, registering
		 * 	a fresh one if this token's category has not been encountered
		 * 	so far
		 */
		private int getType(T token) {
			Cat cat = lexer.category(token);
			@Nullable Integer t = tokenTypes.get(cat);
			if (t != null) return t;
			final int type = nextType++;
			tokenTypes.put(cat, type);
			return type;
		}

		/**
		 * @param token
		 * @return the color for this token's category, registering
		 *  a fresh one if this token's category has not been encountered
		 *  so far
		 */
		private String getColor(T token) {
			Cat cat = lexer.category(token);
			@Nullable String s = tokenColors.get(cat);
			if (s != null) return s;
			final int color = nextColor++;
			final String res = COLORS[color];
			tokenColors.put(cat, res);
			if (nextColor == COLORS.length) nextColor = 0;
			return res;
		}
		
		/**
		 * Record a fresh location in this accumulator for later highlighting
		 * 
		 * @param loc
		 */
		private void record(Location<T> loc) {
			// Array of locations is sorted and locations are disjoint
			// Most frequent case is the new location can be appended at the end,
			// otherwise we need to look for insertion point. We'll work our way
			// from the end (no need to use dichotomy or anything, this will be
			// a super rare case anyway)
			if (locations.size() >= 1 &&
				locations.get(locations.size() - 1).end() > loc.start.offset) {
				// For now simply abort, it requires a big mess-up to make this
				// occur, or something like \include directives
				// TODO: deal with this in a more gracious way, for instance
				// 	recording a special location for overlaps
				throw new IllegalStateException("Encountered non-monotonic location after "
					+ locations.get(locations.size() - 1).end() + ": either positions were "
					+ " badly tampered with in the lexer's semantic actions, or the lexer "
					+ " switches input streams along the way. The token visualizer "
					+ " does not support such features.");
			}
			locations.add(loc);
		}
		
		/**
		 * Consumes one more token from the underlying lexing buffer,
		 * registering the result as adequate.
		 * 
		 * @return {@code false} if the tokenization should be aborted,
		 * 	{@code true} otherwise
		 */
		private boolean next() {
			try {
				final T tok = lexer.entry(lexbuf);
				if (lexer.halt(tok)) return false;
				++numTokens;
				@SuppressWarnings("unused")
				final int type = getType(tok);
				final String color = getColor(tok);
				final Location<T> loc =
					new Token<>(lexbuf.getLexemeStart(), lexbuf.getLexemeEnd(), tok, color);
				record(loc);
				return true;
			}
			catch (LexicalError e) {
				// Record the location of the error (if it had one)
				if (e.pos == null) return false;
				final Location<T> loc = new LexError<>(e, lexbuf.getLexemeEnd());
				record(loc);
				return false;
			}
		}
		
		/**
		 * Proceeds to tokenize input and record locations to highlight,
		 * based on the configuration described in the given {@link #lexer lexing interface}.
		 * <p>
		 * After this method has completed, it is guaranteed that the input has
		 * been tokenized until either the {@link LexerInterface#halt(Object) halting
		 * condition} has been met, or a lexical error has occurred. In any case,
		 * the following properties will hold:
		 * <ul>
		 * <li> the {@link #locations} are sorted and disjoint;
		 * <li> the {@link #tokenTypes} and {@link #tokenColors} have the same domains;
		 * <li> for each token in {@link #locations}, the token's category is 
		 * 		mapped in {@link #tokenTypes} and {@link #tokenColors};
		 * <li> the {@link #tokenTypes} map is bijective, but the {@link #tokenColors}
		 * 		is not necessarily, as colors may be reused if too many categories
		 * 		are encountered.
		 * </ul>
		 */
		void run() {
			while (next());
		}
	}
	
	/**
	 * A container class to record the contents to tokenize, as well
	 * as the total number of lines in the contents.
	 * <p>
	 * By convention, an empty string will have one line.
	 * 
	 * @author Stéphane Lescuyer
	 */
	private static final class Contents {
		final String contents;
		final int lines;
		
		Contents(String contents, int lines) {
			this.contents = contents;
			this.lines = lines;
		}
		
		@Override
		public String toString() {
			return "[" + lines + " lines]\n" + contents;
		}
	}
	
	/**
	 * Reads the totality of {@code input} and return its contents.
	 * 
	 * @param input
	 * @return the description of input's contents
	 * @throws IOException
	 */
	private static Contents contents(Reader input) throws IOException {
		StringBuilder buf = new StringBuilder();
		int lines = 1;
		int c;
		read:
		while ((c = input.read()) != -1) {
			buf.append((char) c);
			if (c == '\r') {
				++lines;
				// try and eat a line feed as well, beware
				// there might be several carriage returns in a row
				while (true) {
					c = input.read();
					if (c == -1) break read;
					buf.append((char) c);
					if (c != '\r') continue read;
					++lines;
				}
			}
			else if (c == '\n') {
				++lines;
			}
		}
		return new Contents(buf.toString(), lines);
	}
	
	private static void save(Writer output, String contents) throws IOException {
		output.append(contents);
		output.flush();
	}

	// Convenient wrappers built on top of CodeBuilder to output
	// a CSS style description
	
	private static void startClass(CodeBuilder buf, String className) {
		buf.emit(className).emit(" {").incrIndent().newline();
	}
	private static void property(CodeBuilder buf, String name, String val) {
		buf.emit(name).emit(": ").emit(val).emit(";");
	}
	private static void properties(CodeBuilder buf, @NonNull String... strings) {
		if (strings.length % 2 != 0)
			throw new IllegalArgumentException();
		int i = 0;
		while (i < strings.length) {
			if (i > 0)
				buf.newline();
			property(buf, strings[i], strings[i + 1]);
			i += 2;
		}
	}
	private static void closeClass(CodeBuilder buf) {
		buf.decrIndent().newline().emitln("}").newline();
	}
	private static void cssClass(CodeBuilder buf, String name, @NonNull String...strings) {
		startClass(buf, name);
		properties(buf, strings);
		closeClass(buf);
	}
	
	/**
	 * Outputs the {@code <style>} node for a standalone HTML page
	 * displaying the result of a tokenization. It contains generic
	 * styles as well as highlighting styles for every token style
	 * encountered in {@code acc}.
	 * 
	 * @param buf	the code builder to output to
	 * @param acc
	 */
	private static <L extends LexBuffer, T, Cat>
	void emitStyle(CodeBuilder buf, Acc<L, T, Cat> acc) {
		buf.emit("<style>").incrIndent().newline();
		
		// body class
		cssClass(buf, "body",
			"background-color", "BlanchedAlmond");
		// h1 class
		cssClass(buf, "h1",
			"color", "white",
			"text-align", "center");
		// input class
		cssClass(buf, ".input",
		    "white-space", "pre",
		    "font-family", "monospace",
			"background-color", "white",
			"padding", "5px",
			"margin-left", "20px",
			"margin-right", "20px",
			"border", "2px solid black");
		// bol class
		cssClass(buf, ".bol",
			"color", "gray",
			"margin-right", "5px");
		// tok class
		cssClass(buf, ".tok",
			"visibility", "hidden",
			// "width", "120px",
			"background-color", "black",
			"color", "#fff",
			"text-align", "center",
			"padding", "5px 5px",
			"border-radius", "6px",
			"position", "absolute",
			"z-index", "1",
			"display", "block",
			"bottom", "150%",
			"left", "-5px"
			// "margin-left", "-60px"
			);
		cssClass(buf, ".tok::after",
			"content", "\"\"",
			"position", "absolute",
			"top", "100%",
			"left", "0%",
			"text-align", "left",
			"margin-left", "5px",
			"border-width", "5px",
			"border-style", "solid",
			"border-color", "black transparent transparent transparent");
		// tokenerr class
		cssClass(buf, ".tokenerr",
			"background-color", "black",
			"border-color", "red",
			"color", "white");
		cssClass(buf, ".tokenerr:hover .tok",
			"visibility", "visible");
		// all token classes
		StringBuilder allTokens = new StringBuilder(".tokenerr");
		for (Map.Entry<Cat, String> entry : acc.tokenColors.entrySet()) {
			Cat cat = entry.getKey();
			String color = entry.getValue();
			// acc.tokenColors and acc.tokenTypes should always have the same domain
			int tty = Nulls.ok(acc.tokenTypes.get(cat));
			String tok = ".token" + tty;
			cssClass(buf, tok,
				"background-color", color,
				"border-color", color);
			cssClass(buf, tok + ":hover .tok",
				"visibility", "visible");
			allTokens.append(", ").append(tok);
		}
		// base class for all tokens
		cssClass(buf, allTokens.toString(),
			"display", "inline-block",
			"position", "relative",
			"vertical-align", "top",
			"border-width", "1px",
			"border-style", "solid",
			"border-radius", "4px");
		
		buf.decrIndent().newline();
		buf.emit("</style>").newline();
	}
	
	/**
	 * A helper class which outputs unconstrained text to an HTML output,
	 * taking care of escaping HTML special characters, and also handling
	 * line counting/displaying when in tokenized output.
	 * <p>
	 * The implementation strives to avoid copies of sub-strings between
	 * the original input and the code builder, and works in a single-pass.
	 * 
	 * @author Stéphane Lescuyer
	 */
	private static final class HtmlEscaper {
		// The complete input which was tokenized
		private final String input;
		// The maximum number of digits to represent the lines in {@code input}.
		// This is needed to right-justify line numbers in the input view.
		private final int digits;
		
		// The code builde to output to
		private final CodeBuilder buf;
		// The current input line being displayed
		private int curLine;
		
		/**
		 * Initializes a fresh HTML escape for the given contents.
		 * <b>Must be called right after the internal {@code <input>}
		 *  HTML node has been entered, as the first thing it does
		 *  is print the first line number.
		 * <b>
		 * 
		 * @param contents
		 * @param buf
		 */
		HtmlEscaper(Contents contents, CodeBuilder buf) {
			this.input = contents.contents;
			// 1 -> 1 .. 9 -> 1, 10 -> 2 ...
			this.digits = (int) (Math.floor(Math.log10(contents.lines) + 1));
			this.curLine = 1;
			this.buf = buf;
			this.bol();
		}
		
		/**
		 * Called when encountering a line break in tokenized output
		 */
		private void eol() {
			++curLine;
		}
		
		/**
		 * Called when starting a new line in decorating tokenized output
		 */
		private void bol() {
			buf.emit("<span class=\"bol\">");
			String d = Integer.toString(curLine);
			int pad = digits - d.length();
			for (int i = 0; i < pad; ++i)
				buf.emit(' ');
			buf.emit(d);
			buf.emit(" </span>");
		}

		/**
		 * Outputs the given character sequence to the HTML output, taking care
		 * of escaping special characters. If {@code count} is {@code true},
		 * line breaks will increment the internal line counter; additionally,
		 * if {@code decorate} is {@code true}, line breaks will be followed
		 * with a line count display. In particular, {@code decorate} will be
		 * ignored if {@code count} is {@code false}.
		 * 
		 * @param seq		the sequence to escape and output
		 * @param decorate  whether line numbers should be displayed (requires {@code count})
		 * @param count		whether line breaks increment the line counter
		 */
		private void htmlEscape(CharSequence seq, boolean decorate, boolean count) {
			final int l = seq.length();
			for (int i = 0; i < l; ++i) {
				char c = seq.charAt(i);
				switch (c) {
				case '<':
					buf.emit("&lt;");
					break;
				case '>':
					buf.emit("&gt;");
					break;
				case '&':
					buf.emit("&amp;");
					break;
				case '\'':
					buf.emit("&#39;");
					break;
				case '"':
					buf.emit("&quot;");
					break;
				case '\r':
					buf.emit(c);
					// try and eat the companion LF if any
					if (i + 1 < l && seq.charAt(i + 1) == '\n') {
						buf.emit('\n');
						++i;
					}
					if (count) {
						eol();
						if (decorate) bol();
					}
					break;
				case '\n':
					// previous character could not be a CR
					buf.emit(c);
					if (count) {
						eol();
						if (decorate) bol();
					}
					break;
				default:
					buf.emit(c);
				}
			}
		}

		/**
		 * Outputs part of the tokenized output which is not part
		 * of any {@code <div>} token.
		 * 
		 * @param from
		 * @param to
		 */
		void untagged(int from, int to) {
			htmlEscape(input.subSequence(from, to), true, true);
		}
		
		/**
		 * Outputs part of the tokenized output which is part of
		 * some {@code <div>} token. Line numbers will not be displayed
		 * for internal line breaks inside this sequence.
		 * 
		 * @param from
		 * @param to
		 */
		void tagged(int from, int to) {
			htmlEscape(input.subSequence(from, to), false, true);
		}
		
		/**
		 * Outputs some string which does not belong to tokenized output.
		 * It does require line counting, nor line count display. 
		 * 
		 * @param s
		 */
		void escape(String s) {
			htmlEscape(s, false, false);
		}
	}
	

	/**
	 * Emits the body of a standalone HTML page displaying the tokenization
	 * results described in {@code acc}. The various recorded {@link Location}s
	 * are highlighted depending on their associated token.
	 * 
	 * @param buf
	 * @param inputName
	 * @param inputContents
	 * @param acc
	 */
	private static <L extends LexBuffer, T, Cat>
	void emitBody(CodeBuilder buf,
		String inputName, Contents inputContents, Acc<L, T, Cat> acc) {
		buf.emit("<body>").incrIndent().newline();
		buf.emit("<h2>").emit("Result of lexical analysis for file ")
			.emit(inputName).emitln("</h2>");
		
		buf.emit("<div class=\"input\">");
		buf.emit("\n"); // no indent on purpose
		HtmlEscaper escaper = new HtmlEscaper(inputContents, buf);
		int curOffset = 0;
		for (Location<T> loc_ : acc.locations) {
			escaper.untagged(curOffset, loc_.start.offset);
			if (loc_ instanceof LexError<?>) {
				LexError<?> loc = (LexError<?>) loc_;
				buf.emit("<div class=\"tokenerr\">");
				escaper.tagged(loc.start.offset, loc.end());
				buf.emit("<span class=\"tok\">");
				escaper.escape(loc.description());
				buf.emit("</span></div>");
			}
			else if (loc_ instanceof Token<?>) {
				Token<T> loc = (Token<T>) loc_;
				T tok = loc.token;
				// acc.tokenTypes should contain all the categories of the encountered tokens
				int tty = Nulls.ok(acc.tokenTypes.get(acc.lexer.category(tok)));
				buf.emit("<div class=\"token" + tty + "\">");
				escaper.tagged(loc.start.offset, loc.end());
				buf.emit("<span class=\"tok\">");
				escaper.escape(loc.description());
				buf.emit("</span></div>");
			}
			else {
				// Impossible for now
			}
			curOffset = loc_.end();
		}
		escaper.untagged(curOffset, inputContents.contents.length());
		buf.newline().emit("</div>");
		
		buf.decrIndent().newline().emit("</body>");
	}

	/**
	 * Outputs to the given writer a stand-alone HTML page which
	 * displays the tokenization of the given {@code input} contents.
	 * The tokenization is performed based on the lexing interface
	 * described by {@code lexer}.
	 * 
	 * @param lexer			the lexing interface used to tokenize and highlight
	 * @param inputName		the name of the input stream
	 * @param input			the input stream to read from
	 * @param output		the output stream to write the HTML page to
	 * @throws IOException
	 */
	public static <L extends LexBuffer, T, Cat>
	void visualize(LexerInterface<L, T, Cat> lexer,
		String inputName, Reader input, Writer output) throws IOException {
		
		Contents inputContents = contents(input);
		
		StringReader replay = new StringReader(inputContents.contents);
		L lexbuf = lexer.makeLexer(inputName, replay);
		Acc<L, T, Cat> acc = new Acc<>(lexbuf, lexer);
		acc.run();
		
		CodeBuilder buf = new CodeBuilder(0);
		buf.emitln("<!DOCTYPE html>");
		buf.emit("<html>").incrIndent().newline();
		emitStyle(buf, acc);
		emitBody(buf, inputName, inputContents, acc);
		buf.decrIndent().newline().emitln("</html>");
		
		save(output, buf.contents());
	}

	/**
	 * Outputs to the given writer a stand-alone HTML page which
	 * displays the tokenization of the given {@code input} string's contents.
	 * The tokenization is performed based on the lexing interface
	 * described by {@code lexer}.
	 * 
	 * @param lexer			the lexing interface used to tokenize and highlight
	 * @param input			the input string to tokenize
	 * @param output		the output stream to write the HTML page to
	 * @throws IOException
	 */
	public static <L extends LexBuffer, T, Cat>
	void string(LexerInterface<L, T, Cat> lexer,
		String input, Writer output) throws IOException {
		visualize(lexer, "raw string", new StringReader(input), output);
	}

	/**
	 * Outputs to the given {@code output} file a stand-alone HTML page which
	 * displays the tokenization of the given {@code input} file contents.
	 * The tokenization is performed based on the lexing interface
	 * described by {@code lexer}.
	 * 
	 * @param lexer			the lexing interface used to tokenize and highlight
	 * @param input			the name of the input file
	 * @param output		the name of the output file to write the HTML page to
	 * @throws IOException
	 */
	public static <L extends LexBuffer, T, Cat>
	void file(
		LexerInterface<L, T, Cat> lexer,
		String input, String output) throws IOException {
		File inputFile = new File(input);
		File outputFile = new File(output);
		try (FileReader inputReader = new FileReader(inputFile);
				BufferedReader inputBufReader = new BufferedReader(inputReader);
				FileWriter outputWriter = new FileWriter(outputFile);
				BufferedWriter outputBufWriter = new BufferedWriter(outputWriter)) {
			visualize(lexer, input, inputBufReader, outputBufWriter);
		}
	}
}
