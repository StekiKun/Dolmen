##Â Notes about running the generated test parsers on citylots.json

* citylots.json is a very large JSON file (180MB), with
  one of the arrays in there having 200k+ elements. It is
  used to test how analyzers generated by Dolmen scale,
  if they are stack-safe, etc.

* JSonLW is designed to test the speed of the lexing/parsing and just that.
  In particular, tokens are not valued (so strings and numerals are just
  analyzed lexically and their value is _then_ thrown away), and the parser
  has no semantic action other than returning from the rules.
  All in all, it is more _validation_ than parsing since no AST is built
  but it is relevant because otherwise the parsing becomes just marginal
  in comparison to the allocation of the huge AST that would come from
  parsing citylots.json.
  
  Avg Time: 2.350s
  (NB: the same validator in Menhir, compiled in native, takes around 2.5s
  	   so it seems we're in the same ballpark. It's impossible to test
  	   in JavaCC because of SOs. Table-based will not have SOs but will
  	   probably be slower?)

* JSon is a regular parser which constructs a JSon AST, so string
  and numeral tokens are valued, and the parser builds stuff in the
  semantic actions. Whether it includes position tracking or not
  is kind of irrelevant because the AST does  not keep position
  information. In fact, this is relevant because it means position
  tracking is neglectible in comparison to building the AST (in that
  case of a large file with limited width/depth of nodes, at least,
  and with proper care to make the position tracking in right-recursive
  rules tail-recursive).
  
  Avg Time: ~20s
  NB: about 13750000 distinct nodes are created in the AST
      (and the literals true, false, null are shared so there are 
       in reality way more actual nodes than that ; we could share
       even more nodes by interning string-nodes since the same string
       attributes are repeated everywhere in the JSon file)
  NB: the same parser in Menhir takes around 7s in native, and 15s
  	  in bytecode so there's something going on. There is definitely
  	  more overhead in the Java representation of the AST but does it
  	  explain everything? 

* JSonPos is the same parser as JSon except that it uses position
  tracking to decorate *every node* in the constructed AST with
  its position (its _range_ more exactly) in the source file. As
  noted above, position tracking in itself is relatively cheap
  but the cost of saving 15Millions * 2 positions in the AST
  is a showstopper. At this point, the shallow size of a LexBuffer.Position
  is 32 bytes: 
    - 2 pointers (Class object + filename String) on a 64bit arch
    - 3 ints
    - 4bytes of what?.. alignment padding, is it?
  + the two pointers to each position in each node, that accounts
  for a required total of around (16B  + 64B) * 15.10^6 = 1.5GB,
  just for the positions.
  It is possible to get rid of 300Mb of pointers by inlining the positions
  in the nodes, and one of the int's in positions could probably be
  turned into a short (saving ~60Mb), but this is definitely an issue.
  
  NB: If instead of two positions, we keep two 32-bit offsets
  in each node, the parsing of citylots.json does indeed complete
  successfully, in slightly less than 30s. Main cost is allocation
  though, and has nothing to do with Dolmen. 